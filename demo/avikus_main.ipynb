{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/obss/sahi/blob/main/demo/inference_for_yolov5.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Install latest version of SAHI and YOLOv5:\n",
    "- pip가 아닌 현재 sahi, yolov5 소스 기반에서 install을 해야만 한다 (커스텀 코드 변경)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U sahi yolov5 scikit-image imagecodecs pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yyj/sahi/demo'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Export frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터를 영상으로 받았을 경우, 매 every 변수마다 프레임을 추출해서 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_frame(video_path, frame_dir, overwrite=False, start=-1, end=-1, every=1):\n",
    "    \"\"\"\n",
    "    Extract frames from a video using OpenCVs VideoCapture\n",
    "    :param video_path: path of the video\n",
    "    :param frames_dir: the directory to save the frames\n",
    "    :param overwrite: to overwrite frames that already exist?\n",
    "    :param start: start frame\n",
    "    :param end: end frame\n",
    "    :param every: frame spacing\n",
    "    :return: count of images saved\n",
    "    \"\"\"\n",
    "    \n",
    "    video_path = os.path.normpath(video_path)\n",
    "    frame_dir = os.path.normpath(frame_dir)\n",
    "    \n",
    "    video_dir, video_filename = os.path.split(video_path)\n",
    "    video_filename = os.path.splitext(video_filename)[0]\n",
    "    assert os.path.exists(video_path)\n",
    "    \n",
    "    capture = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if start < 0:\n",
    "        start = 0\n",
    "    if end < 0:\n",
    "        end = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    capture.set(1, start)\n",
    "    frame = start\n",
    "    while_safety = 0\n",
    "    saved_count = 0 \n",
    "    \n",
    "    while frame < end:\n",
    "        _, image = capture.read()\n",
    "        \n",
    "        if while_safety > 10:  # break the while if our safety maxs out at 10\n",
    "            break\n",
    "            \n",
    "        if image is None:\n",
    "            while_safety += 1\n",
    "            continue\n",
    "            \n",
    "        if frame % every == 0:\n",
    "            while_safety = 0\n",
    "            \n",
    "            save_path = os.path.join(frame_dir, video_filename, \"{:010d}.jpg\".format(frame))\n",
    "            \n",
    "            if not os.path.exists(os.path.join(frame_dir, video_filename)):\n",
    "                os.makedirs(os.path.join(frame_dir, video_filename))\n",
    "            if not os.path.exists(save_path) or overwrite:\n",
    "                cv2.imwrite(save_path, image)\n",
    "                saved_count += 1\n",
    "                \n",
    "        frame += 1\n",
    "        \n",
    "    capture.release()\n",
    "        \n",
    "    return saved_count;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSL\t\t fll_221016.mp4       log_221017_2\r\n",
      "export_frame\t FLL_VAL\t      log_221017_2_narrow\r\n",
      "FL_221017_2.mp4  FLL_VAL_OLD\t      models\r\n",
      "FL_221017.mp4\t FLL_VAL.zip\t      new_train_from_val.zip\r\n",
      "FL_221021.mp4\t hf_spaces_badge.svg  save_test\r\n",
      "FL_221022_2.mp4  lidar01.mp4\t      save_val\r\n",
      "FL_221022.mp4\t lidar02.mp4\t      sliced_inference.gif\r\n",
      "fll_221014.mp4\t log\r\n",
      "fll_221015.mp4\t log_221017\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../resources/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1025"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_path = \"../resources/FL_221022_2.mp4\"\n",
    "frame_dirs = \"../resources/export_frame\"\n",
    "\n",
    "extract_frame(video_path, frame_dirs, every=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrange an instance segmentation model for test\n",
    "from sahi.utils.yolov5 import (\n",
    "    download_yolov5s6_model,\n",
    ")\n",
    "\n",
    "# import required functions, classes\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.utils.cv import read_image\n",
    "from sahi.utils.file import download_from_url\n",
    "from sahi.predict import get_prediction, get_sliced_prediction, predict\n",
    "from IPython.display import Image\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_m_path = '../resources/models/yolov5m.pt'\n",
    "fll_model_221007_path = '../resources/models/221007/best.pt'\n",
    "fll_model_221012_path = '../resources/models/221012/best.pt'\n",
    "fll_model_221014_path = '../resources/models/221014/best.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Standard Inference with a YOLOv5 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instantiate a detection model by defining model weight path and other parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_base_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov5',\n",
    "    model_path=coco_m_path,\n",
    "    confidence_threshold=0.25,\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "fll_model_221007 = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov5',\n",
    "    model_path=fll_model_221007_path,\n",
    "    confidence_threshold=0.25,\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "fll_model_221012 = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov5',\n",
    "    model_path=fll_model_221012_path,\n",
    "    confidence_threshold=0.25,\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "fll_model_221014 = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov5',\n",
    "    model_path=fll_model_221014_path,\n",
    "    confidence_threshold=0.25,\n",
    "    device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fll_model_221014\n",
    "model_path = fll_model_221014_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perform prediction by feeding the get_prediction function with an image path and a DetectionModel instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WANGSAN test image path\n",
    "test_image_path = \"../resources/export_frame/lidar01/0000018960.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_prediction(test_image_path, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Or perform prediction by feeding the get_prediction function with a numpy image and a DetectionModel instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = get_prediction(read_image(test_image_path), detection_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualize predicted bounding boxes and masks over the original image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_prediction(read_image(test_image_path), fll_model_221007)\n",
    "result.export_visuals(export_dir=\"demo_data/\", file_name=\"prediction_visual1\")\n",
    "\n",
    "Image(\"demo_data/prediction_visual1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-1. Total Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"yolov5\"\n",
    "model_device = \"cuda:0\" # or 'cpu'\n",
    "model_confidence_threshold = 0.4\n",
    "\n",
    "slice_height = 512\n",
    "slice_width = 512\n",
    "overlap_height_ratio = 0.2\n",
    "overlap_width_ratio = 0.2\n",
    "\n",
    "source_image_dir = \"../resources/FLL_VAL/images/\"\n",
    "source_label_dir = \"../resources/FLL_VAL/labels/\"\n",
    "\n",
    "no_sliced_prediction = False\n",
    "no_standard_prediction = False\n",
    "custom_slice_mode=1\n",
    "custom_slice_x_start=200\n",
    "custom_slice_y_start=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 함수 CALL 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dataset_json_path: str\n",
    "    If coco file path is provided, detection results will be exported in coco json format.\n",
    "\"\"\"\n",
    "\n",
    "result = predict(\n",
    "    model_type=model_type,\n",
    "    model_path=model_path,\n",
    "    model_device=model_device,\n",
    "    model_confidence_threshold=model_confidence_threshold,\n",
    "    no_sliced_prediction=no_sliced_prediction,\n",
    "    no_standard_prediction=no_standard_prediction,\n",
    "    source=source_image_dir,\n",
    "    slice_height=slice_height,\n",
    "    slice_width=slice_width,\n",
    "    overlap_height_ratio=overlap_height_ratio,\n",
    "    overlap_width_ratio=overlap_width_ratio,\n",
    "    custom_slice_mode=custom_slice_mode,\n",
    "    custom_slice_x_start=custom_slice_x_start,\n",
    "    custom_slice_y_start=custom_slice_y_start,\n",
    "    return_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-2. Interact show_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = sorted([fn for fn in os.listdir(source_image_dir) if fn.endswith(\"jpg\")])\n",
    "label_files = sorted([fn for fn in os.listdir(source_label_dir) if fn.endswith(\"txt\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_files), len(label_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw bbox util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sahi.utils.cv import Colors\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def visualize_object_predictions(\n",
    "    image: np.array,\n",
    "    object_prediction_list,\n",
    "    rect_th: int = None,\n",
    "    text_size: float = None,\n",
    "    text_th: float = None,\n",
    "    color: tuple = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualizes prediction category names, bounding boxes over the source image\n",
    "    and exports it to output folder.\n",
    "    Arguments:\n",
    "        object_prediction_list: a list of prediction.ObjectPrediction\n",
    "        rect_th: rectangle thickness\n",
    "        text_size: size of the category name over box\n",
    "        text_th: text thickness\n",
    "        color: annotation color in the form: (0, 255, 0)\n",
    "        output_dir: directory for resulting visualization to be exported\n",
    "        file_name: exported file will be saved as: output_dir+file_name+\".png\"\n",
    "        export_format: can be specified as 'jpg' or 'png'\n",
    "    \"\"\"\n",
    "    # deepcopy image so that original is not altered\n",
    "    image = copy.deepcopy(image)\n",
    "    # select predefined classwise color palette if not specified\n",
    "    if color is None:\n",
    "        colors = Colors()\n",
    "    else:\n",
    "        colors = None\n",
    "    # set rect_th for boxes\n",
    "    rect_th = rect_th or max(round(sum(image.shape) / 2 * 0.001), 1)\n",
    "    # set text_th for category names\n",
    "    text_th = text_th or max(rect_th - 1, 1)\n",
    "    # set text_size for category names\n",
    "    text_size = text_size or rect_th / 3\n",
    "    # add bbox and mask to image if present\n",
    "    for object_prediction in object_prediction_list:\n",
    "        # deepcopy object_prediction_list so that original is not altered\n",
    "        object_prediction = object_prediction.deepcopy()\n",
    "\n",
    "        bbox = object_prediction.bbox.to_voc_bbox()\n",
    "        category_name = object_prediction.category.name\n",
    "        score = object_prediction.score.value\n",
    "\n",
    "        # set color\n",
    "        if colors is not None:\n",
    "            color = colors(object_prediction.category.id)\n",
    "        # visualize masks if present\n",
    "        if object_prediction.mask is not None:\n",
    "            # deepcopy mask so that original is not altered\n",
    "            mask = object_prediction.mask.bool_mask\n",
    "            # draw mask\n",
    "            rgb_mask = apply_color_mask(mask, color)\n",
    "            image = cv2.addWeighted(image, 1, rgb_mask, 0.4, 0)\n",
    "        # set bbox points\n",
    "        p1, p2 = (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3]))\n",
    "        # visualize boxes\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            p1,\n",
    "            p2,\n",
    "            color=color,\n",
    "            thickness=rect_th\n",
    "        )\n",
    "        # arange bounding box text location\n",
    "        label = f\"{category_name} {score:.2f}\"\n",
    "        w, h = cv2.getTextSize(label, 0, fontScale=text_size, thickness=text_th)[0]  # label width, height\n",
    "        outside = p1[1] - h - 3 >= 0  # label fits outside box\n",
    "        p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\n",
    "        # add bounding box text\n",
    "        cv2.rectangle(image, p1, p2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            label,\n",
    "            (p1[0], p1[1] - 2 if outside else p1[1] + h + 2),\n",
    "            0,\n",
    "            text_size,\n",
    "            (255, 255, 255),\n",
    "            thickness=text_th,\n",
    "        )\n",
    "        \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FULL INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "@interact(index=(0, len(image_files)-1))\n",
    "def show_sample(index=0):\n",
    "    image_file = image_files[index]\n",
    "    image_path = os.path.join(source_image_dir, image_file)\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    result = get_prediction(image_path, model)\n",
    "    \n",
    "    canvas = visualize_object_predictions(image, result.object_prediction_list)\n",
    "    \n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.imshow(canvas)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FULL INFERENCE: Add confidence_threshold with interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "@interact(index=(0, len(image_files)-1), confidence_score=(0, 1, 0.05))\n",
    "def show_sample(index=0, confidence_score=0.25):\n",
    "    sample_model = AutoDetectionModel.from_pretrained(\n",
    "        model_type='yolov5',\n",
    "        model_path=model_path,\n",
    "        confidence_threshold=confidence_score,\n",
    "        device=\"cuda:0\"\n",
    "    )\n",
    "\n",
    "    image_file = image_files[index]\n",
    "    image_path = os.path.join(source_image_dir, image_file)\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    result = get_prediction(image_path, sample_model)\n",
    "    \n",
    "    canvas = visualize_object_predictions(image, result.object_prediction_list)\n",
    "    \n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.imshow(canvas)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-3. Interact show_samples with no nms AutoShape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov5.models.common import AutoShape, DetectMultiBackend\n",
    "from yolov5.utils.torch_utils import select_device\n",
    "\n",
    "device=\"cuda:0\"\n",
    "autoshape=True\n",
    "\n",
    "device = select_device(device)  # detection model\n",
    "AutoShapeModel = AutoShape(DetectMultiBackend(model_path, device=device, fuse=autoshape))  # for file/URI/PIL/cv2/np inputs and NMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "@interact(index=(0, len(image_files)-1))\n",
    "def show_sample(index=0):\n",
    "    image_file = image_files[index]\n",
    "    image_path = os.path.join(source_image_dir, image_file)\n",
    "    \n",
    "    result = AutoShapeModel(image_path, nms=False)\n",
    "    canvas = result.render()[0]\n",
    "    \n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.imshow(canvas)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMS와 NON_NMS 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "@interact(index=(0, len(image_files)-1))\n",
    "def show_sample(index=0):\n",
    "    image_file = image_files[index]\n",
    "    image_path = os.path.join(source_image_dir, image_file)\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    print(\"Image Path:\", image_path)\n",
    "    result_with_nms = get_prediction(image_path, model)\n",
    "\n",
    "    canvas1 = visualize_object_predictions(image, result_with_nms.object_prediction_list)\n",
    "    \n",
    "    result_with_no_nms = AutoShapeModel(image_path, nms=False)\n",
    "    \n",
    "    canvas2 = result_with_no_nms.render()[0]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(32,32)) \n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "    \n",
    "    axes[0].axis('off')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[0].imshow(canvas1)\n",
    "    axes[1].imshow(canvas2)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-4. Interact show_samples with slice prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Row가 잘 잘라지는지에 대한 검사, Shape도 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_slice_mode=2\n",
    "custom_slice_x_start=640\n",
    "custom_slice_y_start=360\n",
    "slice_size=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "%matplotlib inline \n",
    "from sahi import slicing\n",
    "from sahi.slicing import slice_image\n",
    "\n",
    "slicing.logger.setLevel(slicing.logging.INFO)\n",
    "\n",
    "# single_row_y_start: int = 200,\n",
    "@interact(index=(0, len(image_files)-1), slice_size=(0, 512), overlap_ratio=(0, 0.5, 0.05), single_row_y_start=(0, 512))\n",
    "def visualize_slice_rect(index=0, slice_size=512, overlap_ratio=0.2, single_row_y_start=200):\n",
    "    image_file = image_files[index]\n",
    "    image_path = os.path.join(source_image_dir, image_file)\n",
    "    \n",
    "    res = slice_image(image_path, \n",
    "                      slice_width=slice_size,\n",
    "                      slice_height=slice_size,\n",
    "                      overlap_height_ratio=overlap_ratio,\n",
    "                      overlap_width_ratio=overlap_ratio,\n",
    "                      custom_slice_mode=custom_slice_mode,\n",
    "                      custom_slice_x_start=custom_slice_x_start,\n",
    "                      custom_slice_y_start=custom_slice_y_start,\n",
    "                      verbose=1)\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    image = copy.deepcopy(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    for start_pixel in res.starting_pixels:\n",
    "        cv2.rectangle(image,\n",
    "                      start_pixel,\n",
    "                      [s1+s2 for s1, s2 in zip(start_pixel,[slice_size,slice_size])],\n",
    "                      color=(255, 0, 0),\n",
    "                      thickness=2)\n",
    "    \n",
    "    for s_im in res.images:\n",
    "        print(s_im.shape)\n",
    "    \n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw slice image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "%matplotlib inline \n",
    "from sahi import slicing\n",
    "from sahi.slicing import slice_image\n",
    "\n",
    "slicing.logger.setLevel(slicing.logging.INFO)\n",
    "\n",
    "# single_row_y_start: int = 200,\n",
    "@interact(index=(0, len(image_files)-1), slice_size=(0, 512), overlap_ratio=(0, 0.5, 0.05), single_row_y_start=(0, 512))\n",
    "def visualize_slice_rect(index=0, slice_size=512, overlap_ratio=0.2, single_row_y_start=200):\n",
    "    image_file = image_files[index]\n",
    "    image_path = os.path.join(source_image_dir, image_file)\n",
    "    \n",
    "    res = slice_image(image_path, \n",
    "                      slice_width=slice_size,\n",
    "                      slice_height=slice_size,\n",
    "                      overlap_height_ratio=overlap_ratio,\n",
    "                      overlap_width_ratio=overlap_ratio,\n",
    "                      custom_slice_mode=custom_slice_mode,\n",
    "                      custom_slice_x_start=custom_slice_x_start,\n",
    "                      custom_slice_y_start=custom_slice_y_start,\n",
    "                      verbose=1)\n",
    "    \n",
    "    print(len(res.images))\n",
    "    fig, axes = plt.subplots(nrows=len(res.images)//2+1,ncols=2,figsize=(12,16))\n",
    "    \n",
    "#     plt.subplots_adjust(left=0.05, bottom=0.01, right=0.99, \n",
    "#                     top=0.99, wspace=None, hspace=0.2)\n",
    "    \n",
    "    ax = axes.flatten()\n",
    "    \n",
    "    for img_idx, s_im in enumerate(res.images, 0):\n",
    "#         s_im = cv2.cvtColor(s_im, cv2.COLOR_BGR2RGB)\n",
    "        ax[img_idx].imshow(s_im)\n",
    "        ax[img_idx].axis('off')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact slice inference with slice_size, overlap_ratio, single_row_y_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "@interact(index=(0, len(image_files)-1), slice_size=(0, 512), overlap_ratio=(0, 0.5, 0.05), only_full_inference=(0,1))\n",
    "def show_sample(index=0, slice_size=512, overlap_ratio=0.2, only_full_inference=0):\n",
    "    image_file = image_files[index]\n",
    "    image_path = os.path.join(source_image_dir, image_file)\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    if not only_full_inference:\n",
    "        slice_result = slice_image(image_path, \n",
    "                                  slice_width=slice_size,\n",
    "                                  slice_height=slice_size,\n",
    "                                  overlap_height_ratio=overlap_ratio,\n",
    "                                  overlap_width_ratio=overlap_ratio,\n",
    "                                  custom_slice_mode=custom_slice_mode,\n",
    "                                  custom_slice_x_start=custom_slice_x_start,\n",
    "                                  custom_slice_y_start=custom_slice_y_start,\n",
    "                                  verbose=1)\n",
    "\n",
    "        for start_pixel in slice_result.starting_pixels:\n",
    "            cv2.rectangle(image,\n",
    "                          start_pixel,\n",
    "                          [s1+s2 for s1, s2 in zip(start_pixel,[slice_size,slice_size])],\n",
    "                          color=(255, 255, 0),\n",
    "                          thickness=2)\n",
    "        \n",
    "        result = get_sliced_prediction(image_path,\n",
    "                                       model,\n",
    "                                       slice_height=slice_size,\n",
    "                                       slice_width=slice_size,\n",
    "                                       postprocess_match_threshold=0.5,\n",
    "                                       overlap_height_ratio=overlap_ratio,\n",
    "                                       overlap_width_ratio=overlap_ratio,\n",
    "                                       custom_slice_mode=custom_slice_mode,\n",
    "                                       postprocess_type=\"GREEDYNMM\",\n",
    "                                       custom_slice_x_start=custom_slice_x_start,\n",
    "                                       custom_slice_y_start=custom_slice_y_start\n",
    "                                      )\n",
    "    else:\n",
    "        result = get_prediction(image_path, model)\n",
    "    \n",
    "    canvas = visualize_object_predictions(image, result.object_prediction_list)\n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.imshow(canvas)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Yolo Input Image (LETTERBOX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from yolov5.utils.dataloaders import exif_transpose, letterbox\n",
    "from yolov5.utils.general import make_divisible\n",
    "import torch\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "ims =  random.choice(glob(source_image_dir+\"/*.jpg\"))\n",
    "size = (640, 640)\n",
    "device = \"cuda:0\"\n",
    "\n",
    "n, ims = (1, [ims])  # number, list of images\n",
    "\n",
    "shape1 = [] # image and inference shapes, filenames\n",
    "for i, im in enumerate(ims):\n",
    "    f = f'image{i}'  # filename\n",
    "    if isinstance(im, (str, Path)):  # filename or uri\n",
    "        im, f = Image.open(requests.get(im, stream=True).raw if str(im).startswith('http') else im), im\n",
    "        im = np.asarray(exif_transpose(im))\n",
    "    elif isinstance(im, Image.Image):  # PIL Image\n",
    "        im, f = np.asarray(exif_transpose(im)), getattr(im, 'filename', f) or f\n",
    "    if im.shape[0] < 5:  # image in CHW\n",
    "        im = im.transpose((1, 2, 0))  # reverse dataloader .transpose(2, 0, 1)\n",
    "    im = im[..., :3] if im.ndim == 3 else cv2.cvtColor(im, cv2.COLOR_GRAY2BGR)  # enforce 3ch input\n",
    "    \n",
    "    s = im.shape[:2]  # HWC\n",
    "    g = max(size) / max(s)  # gain\n",
    "    shape1.append([y * g for y in s])\n",
    "    ims[i] = im if im.data.contiguous else np.ascontiguousarray(im)  # update\n",
    "\n",
    "shape1 = [make_divisible(x, 32) for x in np.array(shape1).max(0)] # inf shape\n",
    "x = [letterbox(im, shape1, auto=False)[0] for im in ims]  # pad\n",
    "x = np.ascontiguousarray(np.array(x).transpose((0, 3, 1, 2)))  # stack and BHWC to BCHW\n",
    "x = torch.from_numpy(x).to(torch.device(device)).type(torch.float32) / 255  # uint8 to fp16/32\n",
    "\n",
    "plt.imshow(x[0].permute(1,2,0).cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SAHI Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1. Create Val.json (GT), Start Category id \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_extract(img_dir, label_dir, out_dir):\n",
    "    if os.path.exists(os.path.join(out_dir, 'val.json')):\n",
    "        os.remove(os.path.join(out_dir, 'val.json'))\n",
    "    \n",
    "    licenses = [\n",
    "        {\n",
    "            \"name\": \"\",\n",
    "            \"id\": 0,\n",
    "            \"url\": \"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    info_ = [\n",
    "        {\n",
    "            \"contributor\": \"\",\n",
    "            \"date_created\": \"\",\n",
    "            \"description\": \"\",\n",
    "            \"url\": \"\",\n",
    "            \"version\": \"\",\n",
    "            \"year\": \"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    categories = [\n",
    "        {\n",
    "            \"id\": 0,\n",
    "            \"name\": \"Buoy\",\n",
    "            \"supercategory\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"name\": \"Boat\",\n",
    "            \"supercategory\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"name\": \"Channel Marker\",\n",
    "            \"supercategory\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 3,\n",
    "            \"name\": \"Speed Warning Sign\",\n",
    "            \"supercategory\": \"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    img_idx = 0\n",
    "    annot_idx = 0\n",
    "\n",
    "    imgs_list = []\n",
    "    annots_list = []\n",
    "\n",
    "    for label_file in sorted(os.listdir(label_dir)):\n",
    "        label_file_ = os.path.join(label_dir, label_file)\n",
    "        img_file_ = os.path.join(img_dir, f'{os.path.splitext(label_file)[0]}.jpg')\n",
    "        img = Image.open(img_file_)\n",
    "        image_w, image_h = img.size\n",
    "\n",
    "        imgs_list.append({\n",
    "            'id': img_idx,\n",
    "            'width': image_w,\n",
    "            'height': image_h,\n",
    "            'file_name': f'{os.path.splitext(label_file)[0]}.jpg',\n",
    "            \"license\": 0,\n",
    "            \"flickr_url\": \"\",\n",
    "            \"coco_url\": \"\",\n",
    "            \"date_captured\": 0\n",
    "        })\n",
    "\n",
    "        with open(label_file_, 'r') as label_f:\n",
    "            labels = label_f.readlines()\n",
    "\n",
    "            for label in labels:\n",
    "                cat, xc, yc, label_normalized_w, label_normalized_h = list(map(lambda x: int(x) if len(x) == 1 else float(x), label.split()))\n",
    "                label_w, label_h = image_w * label_normalized_w, image_h * label_normalized_h\n",
    "                xmin, ymin = (image_w * xc) - (label_w / 2), (image_h * yc) - (label_h / 2)\n",
    "                \n",
    "                xmin = 0 if xmin < 0 else xmin\n",
    "                ymin = 0 if ymin < 0 else ymin\n",
    "\n",
    "                annots_list.append({\n",
    "                    'id': annot_idx,\n",
    "                    'image_id': img_idx,\n",
    "                    'category_id': cat,\n",
    "                    'area': int(label_h * label_w),\n",
    "                    'bbox': [\n",
    "                        xmin,\n",
    "                        ymin,\n",
    "                        label_w,\n",
    "                        label_h\n",
    "                    ],\n",
    "                    'iscrowd': 0,\n",
    "                    'attributes': {\n",
    "                        'type': '',\n",
    "                        'occluded': False\n",
    "                    },\n",
    "                    'segmentation': []\n",
    "                })\n",
    "\n",
    "                annot_idx += 1\n",
    "\n",
    "        img_idx += 1\n",
    "\n",
    "    out_dict = {\n",
    "        'licenses': licenses,\n",
    "        'info': info_,\n",
    "        'categories': categories,\n",
    "        'images': imgs_list,\n",
    "        'annotations': annots_list\n",
    "    }\n",
    "    \n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(out_dir, 'val.json'), 'w') as out_f:\n",
    "        print(os.path.join(out_dir, 'val.json'))\n",
    "        json.dump(out_dict, out_f)\n",
    "        \n",
    "    return os.path.join(out_dir, 'val.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_extract(img_dir, label_dir, out_dir)\n",
    "gt_json_path = initial_extract(source_image_dir, source_label_dir, str(Path(source_image_dir).parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GT val json으로 만든 것으로 BBOX 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "coco=COCO(gt_json_path)\n",
    "y_offset=-20\n",
    "\n",
    "@interact(index=(0, len(image_files)-1))\n",
    "def draw_gt_bbox(index=0):\n",
    "    fig, ax = plt.subplots()\n",
    "    img = coco.loadImgs(ids=[index])[-1]\n",
    "    I = cv2.imread(os.path.join(source_image_dir, img['file_name']))\n",
    "    I = cv2.cvtColor(I, cv2.COLOR_BGR2RGB)\n",
    "    ax.imshow(I); plt.axis('off')\n",
    "    annIds = coco.getAnnIds(imgIds=img['id'], iscrowd=None)\n",
    "    anns = coco.loadAnns(annIds)\n",
    "    coco.showAnns(anns, draw_bbox=True)\n",
    "    for i, ann in enumerate(anns):\n",
    "        ax.text(anns[i]['bbox'][0], anns[i]['bbox'][1]+y_offset, anns[i]['category_id'], style='italic', \n",
    "                bbox={'facecolor': 'white', 'alpha': 0.7, 'pad': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2. Predict: Parameter Sweep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from sahi.predict import predict\n",
    "from sahi.scripts.coco_error_analysis import analyse\n",
    "from sahi.scripts.coco_evaluation import evaluate\n",
    "\n",
    "MODEL_TYPE = \"yolov5\"\n",
    "MODEL_PATH = model_path\n",
    "MODEL_CONFIG_PATH = \"\"\n",
    "EVAL_IMAGES_FOLDER_DIR = source_image_dir\n",
    "EVAL_DATASET_JSON_PATH = gt_json_path\n",
    "INFERENCE_SETTING = \"AVIKUS_FL\"\n",
    "EXPORT_VISUAL = False\n",
    "MAX_DETECTIONS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_SETTING_TO_PARAMS = {\n",
    "    \"AVIKUS_FL\": {\n",
    "        \"no_standard_prediction\": False,\n",
    "        \"no_sliced_prediction\": False,\n",
    "        \"slice_size\": 512,\n",
    "        \"overlap_ratio\": 0.15,\n",
    "        \"match_threshold\": 0.5,\n",
    "        \"postprocess_class_agnostic\": False,\n",
    "        \"single_row_y_start\": 200,\n",
    "    },\n",
    "}\n",
    "\n",
    "setting_params = INFERENCE_SETTING_TO_PARAMS[INFERENCE_SETTING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_path=MODEL_PATH,\n",
    "    model_config_path=MODEL_CONFIG_PATH,\n",
    "#     model_confidence_threshold=0.01,\n",
    "    model_device=\"cuda:0\",\n",
    "    source=EVAL_IMAGES_FOLDER_DIR,\n",
    "    no_standard_prediction=setting_params[\"no_standard_prediction\"],\n",
    "    no_sliced_prediction=setting_params[\"no_sliced_prediction\"],\n",
    "#     image_size=None,\n",
    "    slice_height=setting_params[\"slice_size\"],\n",
    "    slice_width=setting_params[\"slice_size\"],\n",
    "    overlap_height_ratio=setting_params[\"overlap_ratio\"],\n",
    "    overlap_width_ratio=setting_params[\"overlap_ratio\"],\n",
    "    postprocess_type=\"GREEDYNMM\",\n",
    "    postprocess_match_metric=\"IOS\",\n",
    "#     postprocess_type=\"NMS\",\n",
    "#     postprocess_match_metric=\"IOU\",\n",
    "    postprocess_match_threshold=setting_params[\"match_threshold\"],\n",
    "    postprocess_class_agnostic=setting_params[\"postprocess_class_agnostic\"],\n",
    "    novisual=not EXPORT_VISUAL,\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    project=\"runs/mAP_TEST\",\n",
    "    name=INFERENCE_SETTING,\n",
    "    visual_bbox_thickness=None,\n",
    "    visual_text_size=None,\n",
    "    visual_text_thickness=None,\n",
    "    visual_export_format=\"png\",\n",
    "    verbose=1,\n",
    "    return_dict=True,\n",
    "    force_postprocess_type=True,\n",
    "    single_row_predict=True,\n",
    "    single_row_y_start=setting_params[\"single_row_y_start\"]\n",
    ")\n",
    "\n",
    "result_json_path = str(Path(result[\"export_dir\"]) / \"result.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_dict = evaluate(\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    result_json_path=result_json_path,\n",
    "    classwise=True,\n",
    "    max_detections=MAX_DETECTIONS,\n",
    "    return_dict=True,\n",
    ")\n",
    "\n",
    "analyse_dict = analyse(\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    result_json_path=result_json_path,\n",
    "    max_detections=MAX_DETECTIONS,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3. Draw TP, FP, FN Bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov5.utils.metrics import box_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_image(preds, targets, match_metric):\n",
    "    tp_boxes = None\n",
    "    fp_boxes = None\n",
    "    fn_boxes = None\n",
    "    iou = box_iou(targets[:, 1:], preds[:, :4])\n",
    "    correct_class = targets[:, 0:1] == preds[:, 5]\n",
    "    x = torch.where((iou >= match_metric) & (correct_class))\n",
    "    if x[0].numel():\n",
    "        matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy()\n",
    "        if x[0].shape[0] > 1:\n",
    "            matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "            matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n",
    "            matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "            matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n",
    "        tp_boxes = preds[matches[:, 1]]\n",
    "        fp_idxs = [idx for idx in range(preds.shape[0]) if idx not in matches[:, 1]]\n",
    "        fp_boxes = preds[fp_idxs]\n",
    "        fn_idxs = [idx for idx in range(targets.shape[0]) if idx not in matches[:, 0]]\n",
    "        fn_boxes = targets[fn_idxs]\n",
    "    else:\n",
    "        fn_boxes = targets\n",
    "    return tp_boxes if tp_boxes is not None else torch.tensor([]), fp_boxes if fp_boxes is not None else torch.tensor(\n",
    "        []), fn_boxes if fn_boxes is not None else torch.tensor([])\n",
    "\n",
    "\n",
    "def eval_one_image(img, preds, targets, conf_metric=0.25, match_metrics=0.5):\n",
    "    \"\"\"\n",
    "    img: torch.Tensor\n",
    "    preds: torch.Tensor[x1, y1, x2, y2, conf, class_id]\n",
    "    targets: torch.Tensor[class_id, x1, y1, x2, y2]\n",
    "    \"\"\"\n",
    "    \n",
    "#     (save_dir / 'tps').mkdir(parents=True, exist_ok=True)\n",
    "#     (save_dir / 'fps').mkdir(parents=True, exist_ok=True)\n",
    "#     (save_dir / 'fns').mkdir(parents=True, exist_ok=True)\n",
    "#     (save_dir / 'combined').mkdir(parents=True, exist_ok=True)\n",
    "#     preds = preds[preds[:, 4] > conf_metric]\n",
    "    tps, fps, fns = process_one_image(preds, targets, match_metrics)\n",
    "    \n",
    "    rect_th = max(round(sum(img.shape) / 2 * 0.001), 1)\n",
    "\n",
    "    # visualize boxes    \n",
    "    if tps.numel():\n",
    "        for tp in tps:\n",
    "            x1, y1, x2, y2, conf, class_id = tp\n",
    "            p1, p2 = (int(x1), int(y1)), (int(x2), int(y2))\n",
    "            cv2.rectangle(\n",
    "                img,\n",
    "                p1,\n",
    "                p2,\n",
    "                color=(255,0,0),\n",
    "                thickness=rect_th\n",
    "            )\n",
    "            label = f\"{int(class_id)} {conf:.2f}\"\n",
    "            text_th = max(rect_th - 1, 3)\n",
    "            text_size = rect_th / 3\n",
    "            w, h = cv2.getTextSize(label, 0, fontScale=text_size, thickness=text_th)[0]\n",
    "            # label fits outside box\n",
    "            outside = p1[1] - h - 3 >= 0\n",
    "            \n",
    "            cv2.putText(\n",
    "                img,\n",
    "                label,  \n",
    "                (p1[0], p1[1] - 12 if outside else p1[1] + h + 2),\n",
    "                0,\n",
    "                text_size,\n",
    "                (255, 255, 255),\n",
    "                thickness=text_th,\n",
    "            )\n",
    "            \n",
    "    if fps.numel():\n",
    "        for fp in fps:\n",
    "            x1, y1, x2, y2, conf, class_id = fp\n",
    "            p1, p2 = (int(x1), int(y1)), (int(x2), int(y2))\n",
    "            cv2.rectangle(\n",
    "                img,\n",
    "                p1,\n",
    "                p2,\n",
    "                color=(0,0,255),\n",
    "                thickness=rect_th\n",
    "            )\n",
    "            label = f\"{int(class_id)} {conf:.2f}\"\n",
    "            text_th = max(rect_th - 1, 3)\n",
    "            text_size = rect_th / 3\n",
    "            w, h = cv2.getTextSize(label, 0, fontScale=text_size, thickness=text_th)[0]\n",
    "            # label fits outside box\n",
    "            outside = p1[1] - h - 3 >= 0\n",
    "            \n",
    "            cv2.putText(\n",
    "                img,\n",
    "                label,  \n",
    "                (p1[0], p1[1] - 2 if outside else p1[1] + h + 2),\n",
    "                0,\n",
    "                text_size,\n",
    "                (255, 255, 255),\n",
    "                thickness=text_th,\n",
    "            )\n",
    "            \n",
    "    if fns.numel():\n",
    "        for fn in fns:\n",
    "            class_id, x1, y1, x2, y2 = fn\n",
    "            p1, p2 = (int(x1), int(y1)), (int(x2), int(y2))\n",
    "            cv2.rectangle(\n",
    "                img,\n",
    "                p1,\n",
    "                p2,\n",
    "                color=(0,255,0),\n",
    "                thickness=rect_th\n",
    "            )\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "@interact(index=(0, len(image_files)-1), slice_size=(0, 512), overlap_ratio=(0, 0.5, 0.05), single_row_y_start=(0, 512), only_full_inference=(0,1))\n",
    "def show_sample(index=1780, slice_size=512, overlap_ratio=0.2, single_row_y_start=200, only_full_inference=0):\n",
    "    image_file = image_files[index]\n",
    "    label_path = os.path.join(source_label_dir, label_files[index])\n",
    "    image_path = os.path.join(source_image_dir, image_file)\n",
    "    image = cv2.imread(image_path)\n",
    "    image_h, image_w, _ = image.shape\n",
    "    \n",
    "    \n",
    "    if not only_full_inference:\n",
    "        slice_result = slice_image(image_path, \n",
    "                                  slice_width=slice_size,\n",
    "                                  slice_height=slice_size,\n",
    "                                  overlap_height_ratio=overlap_ratio,\n",
    "                                  overlap_width_ratio=overlap_ratio,\n",
    "                                  single_row_y_start=single_row_y_start,\n",
    "                                  single_row_predict=True,\n",
    "                                  verbose=2)\n",
    "\n",
    "\n",
    "        for start_pixel in slice_result.starting_pixels:\n",
    "            cv2.rectangle(image,\n",
    "                          start_pixel,\n",
    "                          [s1+s2 for s1, s2 in zip(start_pixel,[slice_size,slice_size])],\n",
    "                          color=(255, 255, 0),\n",
    "                          thickness=2)\n",
    "\n",
    "        result = get_sliced_prediction(image_path,\n",
    "                                       model,\n",
    "                                       slice_height=slice_size,\n",
    "                                       slice_width=slice_size,\n",
    "                                       postprocess_match_threshold=0.5,\n",
    "                                       overlap_height_ratio=overlap_ratio,\n",
    "                                       overlap_width_ratio=overlap_ratio,\n",
    "                                       single_row_y_start=single_row_y_start,\n",
    "                                       single_row_predict=True,\n",
    "                                       verbose=2\n",
    "                                      )\n",
    "    else:\n",
    "        result = get_prediction(image_path, model)\n",
    "    \n",
    "#     print(result.to_coco_annotations())\n",
    "    preds = [[ann['bbox'][0], \n",
    "              ann['bbox'][1], \n",
    "              ann['bbox'][0]+ann['bbox'][2], \n",
    "              ann['bbox'][1]+ann['bbox'][3],\n",
    "              ann['score'],\n",
    "              ann['category_id']]\n",
    "              for ann in result.to_coco_annotations()]\n",
    "    \n",
    "    targets = []\n",
    "    \n",
    "    with open(label_path, 'r') as label_f:\n",
    "        labels = label_f.readlines()\n",
    "\n",
    "        for label in labels:\n",
    "            cat, xc, yc, label_normalized_w, label_normalized_h = list(map(lambda x: int(x) if len(x) == 1 else float(x), label.split()))\n",
    "            label_w, label_h = image_w * label_normalized_w, image_h * label_normalized_h\n",
    "            xmin, ymin = (image_w * xc) - (label_w / 2), (image_h * yc) - (label_h / 2)\n",
    "\n",
    "            xmin = 0 if xmin < 0 else xmin\n",
    "            ymin = 0 if ymin < 0 else ymin\n",
    "            \n",
    "            targets.append([cat, xmin, ymin, xmin+label_w, ymin+label_h])\n",
    "    \n",
    "    \n",
    "#     canvas = visualize_object_predictions(image, result.object_prediction_list)\n",
    "    canvas = eval_one_image(image, torch.as_tensor(preds), torch.as_tensor(targets))\n",
    "    canvas = cv2.cvtColor(canvas, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.imshow(canvas)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "@interact(index=(0, len(image_files)-1), slice_size=(0, 512), overlap_ratio=(0, 0.5, 0.05), single_row_y_start=(0, 512))\n",
    "def show_sample(index=1780, slice_size=512, overlap_ratio=0.2, single_row_y_start=200):\n",
    "    fig, axes = plt.subplots(nrows=2,ncols=3,figsize=(32,16))\n",
    "    plt.subplots_adjust(left=0.05, bottom=0.01, right=0.99, \n",
    "                    top=0.99, wspace=None, hspace=0.2)\n",
    "    ax = axes.flatten()\n",
    "    \n",
    "    image_file = image_files[index]\n",
    "    label_path = os.path.join(source_label_dir, label_files[index])\n",
    "    image_path = os.path.join(source_image_dir, image_file)\n",
    "    print(image_path)\n",
    "    image = cv2.imread(image_path)\n",
    "    image_h, image_w, _ = image.shape\n",
    "    \n",
    "    slice_result = slice_image(image_path, \n",
    "                              slice_width=slice_size,\n",
    "                              slice_height=slice_size,\n",
    "                              overlap_height_ratio=overlap_ratio,\n",
    "                              overlap_width_ratio=overlap_ratio,\n",
    "                              single_row_y_start=single_row_y_start,\n",
    "                              single_row_predict=True,\n",
    "                              verbose=2)\n",
    "\n",
    "#     for start_pixel in slice_result.starting_pixels:\n",
    "#         cv2.rectangle(image,\n",
    "#                       start_pixel,\n",
    "#                       [s1+s2 for s1, s2 in zip(start_pixel,[slice_size,slice_size])],\n",
    "#                       color=(255, 255, 0),\n",
    "#                       thickness=2)\n",
    "        \n",
    "#     result = get_sliced_prediction(image_path,\n",
    "#                                    model,\n",
    "#                                    slice_height=slice_size,\n",
    "#                                    slice_width=slice_size,\n",
    "#                                    postprocess_match_threshold=0.5,\n",
    "# #                                    postprocess_type=\"NMM\",\n",
    "#                                    overlap_height_ratio=overlap_ratio,\n",
    "#                                    overlap_width_ratio=overlap_ratio,\n",
    "#                                    single_row_y_start=single_row_y_start,\n",
    "#                                    single_row_predict=True,\n",
    "#                                    verbose=2\n",
    "#                                   )\n",
    "\n",
    "    result = get_prediction(image_path, model)\n",
    "    \n",
    "    canvas = visualize_object_predictions(image, result.object_prediction_list)\n",
    "    ax[5].imshow(canvas)\n",
    "    ax[5].axis('off')\n",
    "        \n",
    "#     print(slice_result.images[0].shape)\n",
    "    img_idx = 0 \n",
    "    for s_im in slice_result.images:\n",
    "        \n",
    "        result = get_prediction(s_im, model)\n",
    "    \n",
    "# #     print(result.to_coco_annotations())\n",
    "#     preds = [[ann['bbox'][0], \n",
    "#               ann['bbox'][1], \n",
    "#               ann['bbox'][0]+ann['bbox'][2], \n",
    "#               ann['bbox'][1]+ann['bbox'][3],\n",
    "#               ann['score'],\n",
    "#               ann['category_id']]\n",
    "#               for ann in result.to_coco_annotations()]\n",
    "    \n",
    "#     targets = []\n",
    "    \n",
    "#     with open(label_path, 'r') as label_f:\n",
    "#         labels = label_f.readlines()\n",
    "\n",
    "#         for label in labels:\n",
    "#             cat, xc, yc, label_normalized_w, label_normalized_h = list(map(lambda x: int(x) if len(x) == 1 else float(x), label.split()))\n",
    "#             label_w, label_h = image_w * label_normalized_w, image_h * label_normalized_h\n",
    "#             xmin, ymin = (image_w * xc) - (label_w / 2), (image_h * yc) - (label_h / 2)\n",
    "\n",
    "#             xmin = 0 if xmin < 0 else xmin\n",
    "#             ymin = 0 if ymin < 0 else ymin\n",
    "            \n",
    "#             targets.append([cat, xmin, ymin, xmin+label_w, ymin+label_h])\n",
    "    \n",
    "    \n",
    "        canvas = visualize_object_predictions(s_im, result.object_prediction_list)\n",
    "#     canvas = eval_one_image(image, torch.as_tensor(preds), torch.as_tensor(targets))\n",
    "        canvas = cv2.cvtColor(canvas, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        ax[img_idx].imshow(canvas)\n",
    "        ax[img_idx].axis('off')\n",
    "        \n",
    "        img_idx += 1\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "coco=COCO(gt_json_path)\n",
    "y_offset=-20\n",
    "\n",
    "@interact(index=(0, len(image_files)-1))\n",
    "def draw_gt_bbox(index=0):\n",
    "    fig, ax = plt.subplots()\n",
    "    img = coco.loadImgs(ids=[index])[-1]\n",
    "    I = cv2.imread(os.path.join(source_image_dir, img['file_name']))\n",
    "    I = cv2.cvtColor(I, cv2.COLOR_BGR2RGB)\n",
    "    ax.imshow(I); plt.axis('off')\n",
    "    annIds = coco.getAnnIds(imgIds=img['id'], iscrowd=None)\n",
    "    anns = coco.loadAnns(annIds)\n",
    "    coco.showAnns(anns, draw_bbox=True)\n",
    "    for i, ann in enumerate(anns):\n",
    "        ax.text(anns[i]['bbox'][0], anns[i]['bbox'][1]+y_offset, anns[i]['category_id'], style='italic', \n",
    "                bbox={'facecolor': 'white', 'alpha': 0.7, 'pad': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dict['eval_results']['bbox_mAP50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### slice_size\n",
    "for p_slice in [256, 384, 512, 640]:\n",
    "    INFERENCE_SETTING_TO_PARAMS[INFERENCE_SETTING][\"slice_size\"] = p_slice\n",
    "    ### overlap_ratio\n",
    "    for p_overlap_ratio in [0, 0.1, 0.2, 0.25]:\n",
    "        INFERENCE_SETTING_TO_PARAMS[INFERENCE_SETTING][\"overlap_ratio\"] = p_overlap_ratio\n",
    "        ### match_threshold\n",
    "        for p_match_threshold in [0.5, 0.7, 0.9]:\n",
    "            INFERENCE_SETTING_TO_PARAMS[INFERENCE_SETTING][\"match_threshold\"] = p_match_threshold\n",
    "            ### postprocess_class_agnostic\n",
    "            for p_postprocess_class_agnostic in [True, False]:\n",
    "                INFERENCE_SETTING_TO_PARAMS[INFERENCE_SETTING][\"postprocess_class_agnostic\"] = p_postprocess_class_agnostic\n",
    "                ### no_sliced_prediction\n",
    "                for p_no_sliced_prediction in [True, False]:\n",
    "                    INFERENCE_SETTING_TO_PARAMS[INFERENCE_SETTING][\"no_sliced_prediction\"] = p_no_sliced_prediction\n",
    "                    ### single_row_y_start\n",
    "                    for p_single_row_y_start in [0, 100, 200]:\n",
    "                        INFERENCE_SETTING_TO_PARAMS[INFERENCE_SETTING][\"single_row_y_start\"] = p_single_row_y_start\n",
    "                        \n",
    "                        setting_info = f\"slice_{p_slice}_overlap_{p_overlap_ratio}_match_{p_match_threshold}_agnostic_{}_no_slice_{}_ystart_{}\"\n",
    "                        \n",
    "                        result = predict(\n",
    "                            model_type=MODEL_TYPE,\n",
    "                            model_path=MODEL_PATH,\n",
    "                            model_config_path=MODEL_CONFIG_PATH,\n",
    "                            model_confidence_threshold=0.01,\n",
    "                            model_device=\"cuda:0\",\n",
    "                            model_category_mapping=None,\n",
    "                            model_category_remapping=None,\n",
    "                            source=EVAL_IMAGES_FOLDER_DIR,\n",
    "                            no_standard_prediction=setting_params[\"no_standard_prediction\"],\n",
    "                            no_sliced_prediction=setting_params[\"no_sliced_prediction\"],\n",
    "                            image_size=None,\n",
    "                            slice_height=setting_params[\"slice_size\"],\n",
    "                            slice_width=setting_params[\"slice_size\"],\n",
    "                            overlap_height_ratio=setting_params[\"overlap_ratio\"],\n",
    "                            overlap_width_ratio=setting_params[\"overlap_ratio\"],\n",
    "                            postprocess_type=\"GREEDYNMM\",\n",
    "                            postprocess_match_metric=\"IOS\",\n",
    "                            postprocess_match_threshold=setting_params[\"match_threshold\"],\n",
    "                            postprocess_class_agnostic=setting_params[\"postprocess_class_agnostic\"],\n",
    "                            novisual=not EXPORT_VISUAL,\n",
    "                            dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "                            project=\"runs/221007\",\n",
    "                            name=setting_info,\n",
    "                            visual_bbox_thickness=None,\n",
    "                            visual_text_size=None,\n",
    "                            visual_text_thickness=None,\n",
    "                            visual_export_format=\"png\",\n",
    "                            verbose=0,\n",
    "                            return_dict=True,\n",
    "                            force_postprocess_type=True,\n",
    "                            single_row_predict=True,\n",
    "                            single_row_y_start=setting_params[\"single_row_y_start\"]\n",
    "                        )\n",
    "                        \n",
    "                        print(\"settings\", setting_info)\n",
    "                        \n",
    "                        result_json_path = str(Path(result[\"export_dir\"]) / \"result.json\")\n",
    "                        \n",
    "                        evaluate_dict = evaluate(\n",
    "                            dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "                            result_json_path=result_json_path,\n",
    "                            classwise=True,\n",
    "                            max_detections=MAX_DETECTIONS,\n",
    "                            return_dict=True,\n",
    "                        )\n",
    "                        \n",
    "                    ### model_confidence_threshold\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "@interact(index=(0, len(image_files)-1), slice_size=(0, 512), overlap_ratio=(0, 0.5, 0.05), single_row_y_start=(0, 512))\n",
    "def show_sample(index=0, slice_size=256, overlap_ratio=0.2, single_row_y_start=200):\n",
    "    image_file = image_files[index]\n",
    "    image_path = os.path.join(source_image_dir, image_file)\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    slice_result = slice_image(image_path, \n",
    "                              slice_width=slice_size,\n",
    "                              slice_height=slice_size,\n",
    "                              overlap_height_ratio=overlap_ratio,\n",
    "                              overlap_width_ratio=overlap_ratio,\n",
    "                              single_row_y_start=single_row_y_start,\n",
    "                              single_row_predict=True,\n",
    "                              verbose=1)\n",
    "\n",
    "    for start_pixel in slice_result.starting_pixels:\n",
    "        cv2.rectangle(image,\n",
    "                      start_pixel,\n",
    "                      [s1+s2 for s1, s2 in zip(start_pixel,[slice_size,slice_size])],\n",
    "                      color=(255, 255, 0),\n",
    "                      thickness=2)\n",
    "        \n",
    "    result = get_sliced_prediction(image_path,\n",
    "                                   model,\n",
    "                                   slice_height=slice_size,\n",
    "                                   slice_width=slice_size,\n",
    "                                   postprocess_match_threshold=0.5,\n",
    "                                   overlap_height_ratio=overlap_ratio,\n",
    "                                   overlap_width_ratio=overlap_ratio,\n",
    "                                   single_row_y_start=single_row_y_start,\n",
    "                                   single_row_predict=True)\n",
    "    \n",
    "    canvas = visualize_object_predictions(image, result.object_prediction_list)\n",
    "    \n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.imshow(canvas)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Predictions are returned as [sahi.prediction.PredictionResult](sahi/prediction.py), you can access the object prediction list as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_prediction_list = result.object_prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_prediction_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ObjectPrediction's can be converted to [COCO annotation](https://cocodataset.org/#format-data) format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_coco_annotations()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ObjectPrediction's can be converted to [COCO prediction](https://github.com/i008/COCO-dataset-explorer) format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_coco_predictions(image_id=1)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ObjectPrediction's can be converted to [imantics](https://github.com/jsbroks/imantics) annotation format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_imantics_annotations()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ObjectPrediction's can be converted to [fiftyone](https://github.com/voxel51/fiftyone) detection format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_fiftyone_detections()[:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec4ff58cab623422d87a0040fa3cf0348daf1830e8a949b925db0b7b4618823f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
