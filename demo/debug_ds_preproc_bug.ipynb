{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff744b4",
   "metadata": {},
   "source": [
    "## 1. GPU 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7847d744",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20657150",
   "metadata": {},
   "source": [
    "## 2. Lib Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9974023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrange an instance segmentation model for test\n",
    "from sahi.utils.yolov5 import (\n",
    "    download_yolov5s6_model,\n",
    ")\n",
    "\n",
    "# import required functions, classes\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.utils.cv import read_image\n",
    "from sahi.utils.file import download_from_url\n",
    "from sahi.predict import get_prediction, get_sliced_prediction, predict\n",
    "from sahi.scripts.coco_error_analysis import analyse\n",
    "from sahi.scripts.coco_evaluation import evaluate\n",
    "from IPython.display import Image\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c8a9ad",
   "metadata": {},
   "source": [
    "## 3. Data & Model Path 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bdb3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image_dir = \"../resources/FLL_VAL/images/\"\n",
    "source_label_dir = \"../resources/FLL_VAL/labels/\"\n",
    "\n",
    "# 960 imgsz로 훈련된 FLL Target\n",
    "fll_model_221024_960_path = '../resources/models/221024_960/best.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ca57f",
   "metadata": {},
   "source": [
    "## 4. Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc0fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fll_model_221024_960 = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov5',\n",
    "    model_path=fll_model_221024_960_path,\n",
    "    confidence_threshold=0.25,\n",
    "    device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d582fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fll_model_221024_960\n",
    "model_path = fll_model_221024_960_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33406e2",
   "metadata": {},
   "source": [
    "## 5. Gt Json 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b601ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_extract(img_dir, label_dir, out_dir):\n",
    "    if os.path.exists(os.path.join(out_dir, 'val.json')):\n",
    "        os.remove(os.path.join(out_dir, 'val.json'))\n",
    "    \n",
    "    licenses = [\n",
    "        {\n",
    "            \"name\": \"\",\n",
    "            \"id\": 0,\n",
    "            \"url\": \"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    info_ = [\n",
    "        {\n",
    "            \"contributor\": \"\",\n",
    "            \"date_created\": \"\",\n",
    "            \"description\": \"\",\n",
    "            \"url\": \"\",\n",
    "            \"version\": \"\",\n",
    "            \"year\": \"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    categories = [\n",
    "        {\n",
    "            \"id\": 0,\n",
    "            \"name\": \"Buoy\",\n",
    "            \"supercategory\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"name\": \"Boat\",\n",
    "            \"supercategory\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"name\": \"Channel Marker\",\n",
    "            \"supercategory\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 3,\n",
    "            \"name\": \"Speed Warning Sign\",\n",
    "            \"supercategory\": \"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    img_idx = 0\n",
    "    annot_idx = 0\n",
    "\n",
    "    imgs_list = []\n",
    "    annots_list = []\n",
    "\n",
    "    for label_file in sorted(os.listdir(label_dir)):\n",
    "        label_file_ = os.path.join(label_dir, label_file)\n",
    "        img_file_ = os.path.join(img_dir, f'{os.path.splitext(label_file)[0]}.jpg')\n",
    "        img = Image.open(img_file_)\n",
    "        image_w, image_h = img.size\n",
    "\n",
    "        imgs_list.append({\n",
    "            'id': img_idx,\n",
    "            'width': image_w,\n",
    "            'height': image_h,\n",
    "            'file_name': f'{os.path.splitext(label_file)[0]}.jpg',\n",
    "            \"license\": 0,\n",
    "            \"flickr_url\": \"\",\n",
    "            \"coco_url\": \"\",\n",
    "            \"date_captured\": 0\n",
    "        })\n",
    "\n",
    "        with open(label_file_, 'r') as label_f:\n",
    "            labels = label_f.readlines()\n",
    "\n",
    "            for label in labels:\n",
    "                cat, xc, yc, label_normalized_w, label_normalized_h = list(map(lambda x: int(x) if len(x) == 1 else float(x), label.split()))\n",
    "                label_w, label_h = image_w * label_normalized_w, image_h * label_normalized_h\n",
    "                xmin, ymin = (image_w * xc) - (label_w / 2), (image_h * yc) - (label_h / 2)\n",
    "                \n",
    "                xmin = 0 if xmin < 0 else xmin\n",
    "                ymin = 0 if ymin < 0 else ymin\n",
    "\n",
    "                annots_list.append({\n",
    "                    'id': annot_idx,\n",
    "                    'image_id': img_idx,\n",
    "                    'category_id': cat,\n",
    "                    'area': int(label_h * label_w),\n",
    "                    'bbox': [\n",
    "                        xmin,\n",
    "                        ymin,\n",
    "                        label_w,\n",
    "                        label_h\n",
    "                    ],\n",
    "                    'iscrowd': 0,\n",
    "                    'attributes': {\n",
    "                        'type': '',\n",
    "                        'occluded': False\n",
    "                    },\n",
    "                    'segmentation': []\n",
    "                })\n",
    "\n",
    "                annot_idx += 1\n",
    "\n",
    "        img_idx += 1\n",
    "\n",
    "    out_dict = {\n",
    "        'licenses': licenses,\n",
    "        'info': info_,\n",
    "        'categories': categories,\n",
    "        'images': imgs_list,\n",
    "        'annotations': annots_list\n",
    "    }\n",
    "    \n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(out_dir, 'val.json'), 'w') as out_f:\n",
    "        print(os.path.join(out_dir, 'val.json'))\n",
    "        json.dump(out_dict, out_f)\n",
    "        \n",
    "    return os.path.join(out_dir, 'val.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a011fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_extract(img_dir, label_dir, out_dir)\n",
    "gt_json_path = initial_extract(source_image_dir, source_label_dir, str(Path(source_image_dir).parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c54ee0",
   "metadata": {},
   "source": [
    "## 6. Eval hyper-param Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669cd56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_SETTING_TO_PARAMS = {\n",
    "    \"AVIKUS_FL\": {\n",
    "        \"model_confidence_threshold\": 0.25,\n",
    "        \"model_device\": \"cuda:0\",\n",
    "        \"image_size\": 960,\n",
    "        \"postprocess_type\": \"GREEDYNMM\",\n",
    "        \"postprocess_match_metric\": \"IOS\",\n",
    "        \"no_standard_prediction\": False, # no FULL inference? \n",
    "        \"no_sliced_prediction\": True, # no Tiling? \n",
    "        \"slice_size\": 512, # Slice size when activate tiling\n",
    "        \"overlap_ratio\": 0.15, # Overlap ratio when activate tiling\n",
    "        \"match_threshold\": 0.5, # Merge match thresh when activate tiling\n",
    "        \"postprocess_class_agnostic\": False,  # class agnostic when activate tiling\n",
    "        \"custom_slice_y_start\": 200,  # Y start point when activate tiling\n",
    "    },\n",
    "}\n",
    "\n",
    "MODEL_TYPE = \"yolov5\" # model type\n",
    "MODEL_PATH = model_path # model path\n",
    "MODEL_CONFIG_PATH = \"\"\n",
    "EVAL_IMAGES_FOLDER_DIR = source_image_dir # source dir \n",
    "EVAL_DATASET_JSON_PATH = gt_json_path # gt json path\n",
    "INFERENCE_SETTING = \"AVIKUS_FL\"\n",
    "EXPORT_VISUAL = False\n",
    "MAX_DETECTIONS = 300\n",
    "\n",
    "setting_params = INFERENCE_SETTING_TO_PARAMS[INFERENCE_SETTING]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d156d93",
   "metadata": {},
   "source": [
    "## 참고. Interact로 인퍼런스 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e53b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sahi.utils.cv import Colors\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def visualize_object_predictions(\n",
    "    image: np.array,\n",
    "    object_prediction_list,\n",
    "    rect_th: int = None,\n",
    "    text_size: float = None,\n",
    "    text_th: float = None,\n",
    "    color: tuple = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualizes prediction category names, bounding boxes over the source image\n",
    "    and exports it to output folder.\n",
    "    Arguments:\n",
    "        object_prediction_list: a list of prediction.ObjectPrediction\n",
    "        rect_th: rectangle thickness\n",
    "        text_size: size of the category name over box\n",
    "        text_th: text thickness\n",
    "        color: annotation color in the form: (0, 255, 0)\n",
    "        output_dir: directory for resulting visualization to be exported\n",
    "        file_name: exported file will be saved as: output_dir+file_name+\".png\"\n",
    "        export_format: can be specified as 'jpg' or 'png'\n",
    "    \"\"\"\n",
    "    # deepcopy image so that original is not altered\n",
    "    image = copy.deepcopy(image)\n",
    "    # select predefined classwise color palette if not specified\n",
    "    if color is None:\n",
    "        colors = Colors()\n",
    "    else:\n",
    "        colors = None\n",
    "    # set rect_th for boxes\n",
    "    rect_th = rect_th or max(round(sum(image.shape) / 2 * 0.001), 1)\n",
    "    # set text_th for category names\n",
    "    text_th = text_th or max(rect_th - 1, 1)\n",
    "    # set text_size for category names\n",
    "    text_size = text_size or rect_th / 3\n",
    "    # add bbox and mask to image if present\n",
    "    for object_prediction in object_prediction_list:\n",
    "        # deepcopy object_prediction_list so that original is not altered\n",
    "        object_prediction = object_prediction.deepcopy()\n",
    "\n",
    "        bbox = object_prediction.bbox.to_voc_bbox()\n",
    "        category_name = object_prediction.category.name\n",
    "        score = object_prediction.score.value\n",
    "\n",
    "        # set color\n",
    "        if colors is not None:\n",
    "            color = colors(object_prediction.category.id)\n",
    "        # visualize masks if present\n",
    "        if object_prediction.mask is not None:\n",
    "            # deepcopy mask so that original is not altered\n",
    "            mask = object_prediction.mask.bool_mask\n",
    "            # draw mask\n",
    "            rgb_mask = apply_color_mask(mask, color)\n",
    "            image = cv2.addWeighted(image, 1, rgb_mask, 0.4, 0)\n",
    "        # set bbox points\n",
    "        p1, p2 = (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3]))\n",
    "        # visualize boxes\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            p1,\n",
    "            p2,\n",
    "            color=color,\n",
    "            thickness=rect_th\n",
    "        )\n",
    "        # arange bounding box text location\n",
    "        label = f\"{category_name} {score:.2f}\"\n",
    "        w, h = cv2.getTextSize(label, 0, fontScale=text_size, thickness=text_th)[0]  # label width, height\n",
    "        outside = p1[1] - h - 3 >= 0  # label fits outside box\n",
    "        p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\n",
    "        # add bounding box text\n",
    "        cv2.rectangle(image, p1, p2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            label,\n",
    "            (p1[0], p1[1] - 2 if outside else p1[1] + h + 2),\n",
    "            0,\n",
    "            text_size,\n",
    "            (255, 255, 255),\n",
    "            thickness=text_th,\n",
    "        )\n",
    "        \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909fafa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "from sahi.slicing import slice_image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "image_files = sorted(os.listdir(source_image_dir))\n",
    "\n",
    "@interact(index=(0, len(image_files)-1),\n",
    "          slice_size=(0, 640),\n",
    "          overlap_ratio=(0, 0.5, 0.05),\n",
    "          custom_slice_x_start=(0, 640),\n",
    "          custom_slice_y_start=(0, 512),\n",
    "          custom_slice_mode=(0,3),\n",
    "          only_full_inference=(0,1))\n",
    "def show_sample(index=0, slice_size=640, overlap_ratio=0.25,\n",
    "                custom_slice_x_start=640, custom_slice_y_start=360, custom_slice_mode=2,\n",
    "                only_full_inference=0):\n",
    "    image_file = image_files[index]\n",
    "    image_path = os.path.join(source_image_dir, image_file)\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    if not only_full_inference:\n",
    "        slice_result = slice_image(image_path, \n",
    "                                  slice_width=slice_size,\n",
    "                                  slice_height=slice_size,\n",
    "                                  overlap_height_ratio=overlap_ratio,\n",
    "                                  overlap_width_ratio=overlap_ratio,\n",
    "                                  custom_slice_x_start=custom_slice_x_start,\n",
    "                                  custom_slice_y_start=custom_slice_y_start,\n",
    "                                  custom_slice_mode=custom_slice_mode,\n",
    "                                  verbose=1)\n",
    "\n",
    "        for start_pixel in slice_result.starting_pixels:\n",
    "            cv2.rectangle(image,\n",
    "                          start_pixel,\n",
    "                          [s1+s2 for s1, s2 in zip(start_pixel,[slice_size,slice_size])],\n",
    "                          color=(255, 255, 0),\n",
    "                          thickness=2)\n",
    "        \n",
    "        result = get_sliced_prediction(image_path,\n",
    "                                       model,\n",
    "                                       slice_height=slice_size,\n",
    "                                       slice_width=slice_size,\n",
    "                                       postprocess_match_threshold=0.5,\n",
    "                                       overlap_height_ratio=overlap_ratio,\n",
    "                                       overlap_width_ratio=overlap_ratio,\n",
    "                                       custom_slice_x_start=custom_slice_x_start,\n",
    "                                       custom_slice_y_start=custom_slice_y_start,\n",
    "                                       custom_slice_mode=custom_slice_mode\n",
    "                                      )\n",
    "    else:\n",
    "        result = get_prediction(image_path, model)\n",
    "    \n",
    "    canvas = visualize_object_predictions(image, result.object_prediction_list)\n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.imshow(canvas)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243af9c2",
   "metadata": {},
   "source": [
    "## 7. Execute Full Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17434604",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_path=MODEL_PATH,\n",
    "    model_config_path=MODEL_CONFIG_PATH,\n",
    "    model_confidence_threshold=setting_params[\"model_confidence_threshold\"],\n",
    "    model_device=setting_params[\"model_device\"],\n",
    "    model_category_mapping=None,\n",
    "    model_category_remapping=None,\n",
    "    source=EVAL_IMAGES_FOLDER_DIR,\n",
    "    no_standard_prediction=setting_params[\"no_standard_prediction\"],\n",
    "    no_sliced_prediction=setting_params[\"no_sliced_prediction\"],\n",
    "    slice_height=setting_params[\"slice_size\"],\n",
    "    slice_width=setting_params[\"slice_size\"],\n",
    "    overlap_height_ratio=setting_params[\"overlap_ratio\"],\n",
    "    overlap_width_ratio=setting_params[\"overlap_ratio\"],\n",
    "    image_size=setting_params[\"image_size\"],\n",
    "    postprocess_type=setting_params[\"postprocess_type\"],\n",
    "    postprocess_match_metric=setting_params[\"postprocess_match_metric\"],\n",
    "    postprocess_match_threshold=setting_params[\"match_threshold\"],\n",
    "    postprocess_class_agnostic=setting_params[\"postprocess_class_agnostic\"],\n",
    "    novisual=not EXPORT_VISUAL,\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    project=\"runs/FLL_FULL_INFERENCE\",\n",
    "    name=INFERENCE_SETTING,\n",
    "    visual_bbox_thickness=None,\n",
    "    visual_text_size=None,\n",
    "    visual_text_thickness=None,\n",
    "    visual_export_format=\"png\",\n",
    "    verbose=2,\n",
    "    return_dict=True,\n",
    "    force_postprocess_type=True,\n",
    "    custom_slice_mode=0,\n",
    "    custom_slice_x_start=0,\n",
    "    custom_slice_y_start=0,\n",
    ")\n",
    "\n",
    "result_json_path = str(Path(result[\"export_dir\"]) / \"result.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b292a9ab",
   "metadata": {},
   "source": [
    "## 8. mAP evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21ca11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dict = evaluate(\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    result_json_path=result_json_path,\n",
    "    classwise=True,\n",
    "    max_detections=MAX_DETECTIONS,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f19033",
   "metadata": {},
   "source": [
    "## 9. Export DSL Standard result.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb9a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsl_standard_result_json = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358dce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dict = evaluate(\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    result_json_path=dsl_standard_result_json,\n",
    "    classwise=True,\n",
    "    max_detections=MAX_DETECTIONS,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313a72d3",
   "metadata": {},
   "source": [
    "## 10. Visualize json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58764610",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dsl_standard_result_json, 'r') as f:\n",
    "    datas = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e66775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "\n",
    "@interact(index=(0, len(image_files)-1))\n",
    "def show_sample(index=0):\n",
    "    infos = [ data for data in datas if data['image_id'] == index ]\n",
    "    image_path = os.path.join(source_image_dir, image_files[index])\n",
    "    image = cv2.imread(image_path)    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    for info in infos:\n",
    "        bbox = list(map(int, info['bbox']))\n",
    "        category_name =  info['category_name']\n",
    "        conf = info['score']\n",
    "        label = f\"{category_name} {conf:.2f}\"\n",
    "        \n",
    "        cv2.rectangle(image,\n",
    "                      bbox[:2],\n",
    "                      [ bbox1 + bbox2 for bbox1, bbox2 in zip(bbox[:2], bbox[2:])],\n",
    "                      color=(255, 0, 0),\n",
    "                      thickness=2)\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            label,  \n",
    "            [bbox[0], bbox[1]-10],\n",
    "            0,\n",
    "            1,\n",
    "            (255, 255, 255),\n",
    "            thickness=3,\n",
    "        )\n",
    "        \n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c23c1af",
   "metadata": {},
   "source": [
    "## 11. Export DSL Preproc Full inference result.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53940b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsl_preproc_full_inference_json = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6950db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dict = evaluate(\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    result_json_path=dsl_preproc_full_inference_json,\n",
    "    classwise=True,\n",
    "    max_detections=MAX_DETECTIONS,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d284e",
   "metadata": {},
   "source": [
    "## 12. Visualize json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e46e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dsl_preproc_full_inference_json, 'r') as f:\n",
    "    datas = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114cab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "\n",
    "@interact(index=(0, len(image_files)-1))\n",
    "def show_sample(index=0):\n",
    "    infos = [ data for data in datas if data['image_id'] == index ]\n",
    "    image_path = os.path.join(source_image_dir, image_files[index])\n",
    "    image = cv2.imread(image_path)    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    for info in infos:\n",
    "        bbox = list(map(int, info['bbox']))\n",
    "        category_name =  info['category_name']\n",
    "        conf = info['score']\n",
    "        label = f\"{category_name} {conf:.2f}\"\n",
    "        \n",
    "        cv2.rectangle(image,\n",
    "                      bbox[:2],\n",
    "                      [ bbox1 + bbox2 for bbox1, bbox2 in zip(bbox[:2], bbox[2:])],\n",
    "                      color=(255, 0, 0),\n",
    "                      thickness=2)\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            label,  \n",
    "            [bbox[0], bbox[1]-10],\n",
    "            0,\n",
    "            1,\n",
    "            (255, 255, 255),\n",
    "            thickness=3,\n",
    "        )\n",
    "        \n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
