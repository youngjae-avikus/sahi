{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eebc0d0",
   "metadata": {},
   "source": [
    "# 1. 비디오 불러와서 프레임 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb07ec09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yyj/sahi/demo'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd9ba4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def extract_frame(video_path, frame_dir, overwrite=False, start=-1, end=-1, every=1):\n",
    "    \"\"\"\n",
    "    Extract frames from a video using OpenCVs VideoCapture\n",
    "    :param video_path: path of the video\n",
    "    :param frames_dir: the directory to save the frames\n",
    "    :param overwrite: to overwrite frames that already exist?\n",
    "    :param start: start frame\n",
    "    :param end: end frame\n",
    "    :param every: frame spacing\n",
    "    :return: count of images saved\n",
    "    \"\"\"\n",
    "    \n",
    "    video_path = os.path.normpath(video_path)\n",
    "    frame_dir = os.path.normpath(frame_dir)\n",
    "    \n",
    "    video_dir, video_filename = os.path.split(video_path)\n",
    "    video_filename = os.path.splitext(video_filename)[0]\n",
    "    assert os.path.exists(video_path)\n",
    "    \n",
    "    capture = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if start < 0:\n",
    "        start = 0\n",
    "    if end < 0:\n",
    "        end = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    print(fps)\n",
    "    \n",
    "    capture.set(cv2.CAP_PROP_FPS,30) \n",
    "    print(capture.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    print(start, end)\n",
    "    capture.set(1, start)\n",
    "    frame = start\n",
    "    while_safety = 0\n",
    "    saved_count = 0 \n",
    "    \n",
    "    while frame < end:\n",
    "        if while_safety > 10:  # break the while if our safety maxs out at 10\n",
    "            break\n",
    "        \n",
    "        _, image = capture.read()\n",
    "        \n",
    "        if frame % every == 0:\n",
    "           \n",
    "            if image is None:\n",
    "                print(\"Image is None\")\n",
    "                while_safety += 1\n",
    "                continue\n",
    "            \n",
    "            while_safety = 0\n",
    "            \n",
    "            save_path = os.path.join(frame_dir, video_filename, \"{:010d}.jpg\".format(saved_count))\n",
    "            \n",
    "            if not os.path.exists(os.path.join(frame_dir, video_filename)):\n",
    "                os.makedirs(os.path.join(frame_dir, video_filename))\n",
    "            if not os.path.exists(save_path) or overwrite:\n",
    "                cv2.imwrite(save_path, image)\n",
    "                saved_count += 1\n",
    "                \n",
    "        frame += 1\n",
    "        \n",
    "    capture.release()\n",
    "        \n",
    "    return saved_count, os.path.join(frame_dir, video_filename);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01b0a080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export_frame\tfll_221016.mp4\t     lidar02.mp4   log_221017_2_narrow\r\n",
      "FL_221017.mp4\tFLL_VAL\t\t     log\t   models\r\n",
      "fll_221014.mp4\thf_spaces_badge.svg  log_221017    save_test\r\n",
      "fll_221015.mp4\tlidar01.mp4\t     log_221017_2  sliced_inference.gif\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../resources/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7439f748",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../resources/FL_221017_2.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m frame_dirs \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../resources/export_frame\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m _, source_image_dir \u001b[38;5;241m=\u001b[39m \u001b[43mextract_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [2], line 20\u001b[0m, in \u001b[0;36mextract_frame\u001b[0;34m(video_path, frame_dir, overwrite, start, end, every)\u001b[0m\n\u001b[1;32m     18\u001b[0m video_dir, video_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(video_path)\n\u001b[1;32m     19\u001b[0m video_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(video_filename)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(video_path)\n\u001b[1;32m     22\u001b[0m capture \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(video_path)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "video_path = \"../resources/FL_221017_2.mp4\"\n",
    "frame_dirs = \"../resources/export_frame\"\n",
    "\n",
    "_, source_image_dir = extract_frame(video_path, frame_dirs, every=30, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62dab927",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image_dir = \"../resources/export_frame/FL_221017_2_narrow//\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f1e191",
   "metadata": {},
   "source": [
    "# 2. 비디오 인퍼런스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c59db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrange an instance segmentation model for test\n",
    "from sahi.utils.yolov5 import (\n",
    "    download_yolov5s6_model,\n",
    ")\n",
    "\n",
    "# import required functions, classes\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.utils.cv import read_image\n",
    "from sahi.utils.file import download_from_url\n",
    "from sahi.predict import get_prediction, get_sliced_prediction, predict\n",
    "from sahi.scripts.coco_error_analysis import analyse\n",
    "from sahi.scripts.coco_evaluation import evaluate\n",
    "from sahi.slicing import slice_image\n",
    "from IPython.display import Image\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "fll_model_221014_path = '../resources/models/221014/best.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b300a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fll_model_221014 = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov5',\n",
    "    model_path=fll_model_221014_path,\n",
    "    confidence_threshold=0.25,\n",
    "    device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e53cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fll_model_221014\n",
    "model_path = fll_model_221014_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be11f620",
   "metadata": {},
   "source": [
    "## Interact로 인퍼런스 샘플 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e88a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sahi.utils.cv import Colors\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def visualize_object_predictions(\n",
    "    image: np.array,\n",
    "    object_prediction_list,\n",
    "    rect_th: int = None,\n",
    "    text_size: float = None,\n",
    "    text_th: float = None,\n",
    "    color: tuple = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualizes prediction category names, bounding boxes over the source image\n",
    "    and exports it to output folder.\n",
    "    Arguments:\n",
    "        object_prediction_list: a list of prediction.ObjectPrediction\n",
    "        rect_th: rectangle thickness\n",
    "        text_size: size of the category name over box\n",
    "        text_th: text thickness\n",
    "        color: annotation color in the form: (0, 255, 0)\n",
    "        output_dir: directory for resulting visualization to be exported\n",
    "        file_name: exported file will be saved as: output_dir+file_name+\".png\"\n",
    "        export_format: can be specified as 'jpg' or 'png'\n",
    "    \"\"\"\n",
    "    # deepcopy image so that original is not altered\n",
    "    image = copy.deepcopy(image)\n",
    "    # select predefined classwise color palette if not specified\n",
    "    if color is None:\n",
    "        colors = Colors()\n",
    "    else:\n",
    "        colors = None\n",
    "    # set rect_th for boxes\n",
    "    rect_th = rect_th or max(round(sum(image.shape) / 2 * 0.001), 1)\n",
    "    # set text_th for category names\n",
    "    text_th = text_th or max(rect_th - 1, 1)\n",
    "    # set text_size for category names\n",
    "    text_size = text_size or rect_th / 3\n",
    "    # add bbox and mask to image if present\n",
    "    for object_prediction in object_prediction_list:\n",
    "        # deepcopy object_prediction_list so that original is not altered\n",
    "        object_prediction = object_prediction.deepcopy()\n",
    "\n",
    "        bbox = object_prediction.bbox.to_voc_bbox()\n",
    "        category_name = object_prediction.category.name\n",
    "        score = object_prediction.score.value\n",
    "\n",
    "        # set color\n",
    "        if colors is not None:\n",
    "            color = colors(object_prediction.category.id)\n",
    "        # visualize masks if present\n",
    "        if object_prediction.mask is not None:\n",
    "            # deepcopy mask so that original is not altered\n",
    "            mask = object_prediction.mask.bool_mask\n",
    "            # draw mask\n",
    "            rgb_mask = apply_color_mask(mask, color)\n",
    "            image = cv2.addWeighted(image, 1, rgb_mask, 0.4, 0)\n",
    "        # set bbox points\n",
    "        p1, p2 = (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3]))\n",
    "        # visualize boxes\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            p1,\n",
    "            p2,\n",
    "            color=color,\n",
    "            thickness=rect_th\n",
    "        )\n",
    "        # arange bounding box text location\n",
    "        label = f\"{category_name} {score:.2f}\"\n",
    "        w, h = cv2.getTextSize(label, 0, fontScale=text_size, thickness=text_th)[0]  # label width, height\n",
    "        outside = p1[1] - h - 3 >= 0  # label fits outside box\n",
    "        p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\n",
    "        # add bounding box text\n",
    "        cv2.rectangle(image, p1, p2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            label,\n",
    "            (p1[0], p1[1] - 2 if outside else p1[1] + h + 2),\n",
    "            0,\n",
    "            text_size,\n",
    "            (255, 255, 255),\n",
    "            thickness=text_th,\n",
    "        )\n",
    "        \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ccd660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "image_files = sorted(os.listdir(source_image_dir))\n",
    "\n",
    "@interact(index=(0, len(image_files)-1), slice_size=(0, 640), overlap_ratio=(0, 0.5, 0.05), single_row_y_start=(0, 512), only_full_inference=(0,1))\n",
    "def show_sample(index=0, slice_size=640, overlap_ratio=0.25, single_row_y_start=200, only_full_inference=0):\n",
    "    image_file = image_files[index]\n",
    "    image_path = os.path.join(source_image_dir, image_file)\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    if not only_full_inference:\n",
    "        slice_result = slice_image(image_path, \n",
    "                                  slice_width=slice_size,\n",
    "                                  slice_height=slice_size,\n",
    "                                  overlap_height_ratio=overlap_ratio,\n",
    "                                  overlap_width_ratio=overlap_ratio,\n",
    "                                  single_row_y_start=single_row_y_start,\n",
    "                                  single_row_predict=True,\n",
    "                                  verbose=1)\n",
    "\n",
    "        for start_pixel in slice_result.starting_pixels:\n",
    "            cv2.rectangle(image,\n",
    "                          start_pixel,\n",
    "                          [s1+s2 for s1, s2 in zip(start_pixel,[slice_size,slice_size])],\n",
    "                          color=(255, 255, 0),\n",
    "                          thickness=2)\n",
    "        \n",
    "        result = get_sliced_prediction(image_path,\n",
    "                                       model,\n",
    "                                       slice_height=slice_size,\n",
    "                                       slice_width=slice_size,\n",
    "                                       postprocess_match_threshold=0.5,\n",
    "                                       overlap_height_ratio=overlap_ratio,\n",
    "                                       overlap_width_ratio=overlap_ratio,\n",
    "                                       single_row_y_start=single_row_y_start,\n",
    "                                       single_row_predict=True)\n",
    "    else:\n",
    "        result = get_prediction(image_path, model)\n",
    "    \n",
    "    canvas = visualize_object_predictions(image, result.object_prediction_list)\n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.imshow(canvas)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b8d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_image_dir = '../resources/log_221017_2_narrow/images/'\n",
    "yolo_label_dir = '../resources/log_221017_2_narrow/labels/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(yolo_image_dir, exist_ok=True)\n",
    "os.makedirs(yolo_label_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a791efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "slice_size=640\n",
    "overlap_ratio=0.25\n",
    "single_row_predict=True\n",
    "single_row_y_start=200\n",
    "postprocess_match_threshold=0.5\n",
    "no_slice_prediction=True\n",
    "cnt=0\n",
    "size=(1920,1080)\n",
    "\n",
    "def convert(size, box):\n",
    "    dw = 1./(size[0])\n",
    "    dh = 1./(size[1])\n",
    "    x = (box[0] + box[1])/2.0 - 1\n",
    "    y = (box[2] + box[3])/2.0 - 1\n",
    "    w = box[1] - box[0]\n",
    "    h = box[3] - box[2]\n",
    "    x = x*dw\n",
    "    w = w*dw\n",
    "    y = y*dh\n",
    "    h = h*dh\n",
    "    return (x,y,w,h)\n",
    "\n",
    "for image_file in tqdm(sorted(glob(source_image_dir+\"/*.jpg\"))):\n",
    "    \n",
    "    shutil.copy(image_file, yolo_image_dir+os.path.basename(image_file))\n",
    "    label_file_name = os.path.splitext(os.path.basename(image_file))[0]+\".txt\"\n",
    "    \n",
    "    if not no_slice_prediction:\n",
    "        result = get_sliced_prediction(image_file,\n",
    "                                       model,\n",
    "                                       slice_height=slice_size,\n",
    "                                       slice_width=slice_size,\n",
    "                                       postprocess_match_threshold=postprocess_match_threshold,\n",
    "                                       overlap_height_ratio=overlap_ratio,\n",
    "                                       overlap_width_ratio=overlap_ratio,\n",
    "                                       single_row_y_start=single_row_y_start,\n",
    "                                       single_row_predict=single_row_predict,\n",
    "                                       verbose=0)\n",
    "    else:\n",
    "        result = get_prediction(image_file, model)\n",
    "\n",
    "    Path(yolo_label_dir+label_file_name).touch()\n",
    "    with open(yolo_label_dir+label_file_name, 'w') as f:    \n",
    "        for object_prediction in result.object_prediction_list:\n",
    "            object_prediction = object_prediction.deepcopy()\n",
    "            bbox = object_prediction.bbox.to_voc_bbox()\n",
    "            category_id = object_prediction.category.id\n",
    "            yolo = convert(size,[bbox[0], bbox[2], bbox[1], bbox[3]])\n",
    "            f.write(f\"{category_id} {yolo[0]} {yolo[1]} {yolo[2]} {yolo[3]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed46c517",
   "metadata": {},
   "source": [
    "## Visualize yolo bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8220e9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAME_TO_ID = {'Buoy': 0, 'Boat': 1, 'Channel Marker': 2, 'Speed Warning Sign': 3}\n",
    "CLASS_ID_TO_NAME = {0: 'Buoy', 1: 'Boat', 2: 'Channel Marker', 3: 'Speed Warning Sign'}\n",
    "SIZE=(1920,1080)\n",
    "\n",
    "def visualize_yolo(image, bboxes, category_ids, color=None):\n",
    "    image = copy.deepcopy(image)\n",
    "    \n",
    "    if color is None:\n",
    "        colors = Colors()\n",
    "    else:\n",
    "        colors = None\n",
    "        \n",
    "    for bbox, category_id in zip(bboxes, category_ids):\n",
    "        class_name = CLASS_ID_TO_NAME[category_id[0]]\n",
    "        \n",
    "        x_center, y_center, w, h = bbox\n",
    "        x_min = int((x_center - w/2) * SIZE[0])\n",
    "        y_min = int((y_center - h/2) * SIZE[1])\n",
    "        x_max = int((x_center + w/2) * SIZE[0])\n",
    "        y_max = int((y_center + h/2) * SIZE[1])\n",
    "\n",
    "        if colors is not None:\n",
    "            color = colors(category_id[0])\n",
    "        \n",
    "        # set rect_th for boxes\n",
    "        rect_th = max(round(sum(image.shape) / 2 * 0.001), 1)\n",
    "        # set text_th for category names\n",
    "        text_th = max(rect_th - 1, 1)\n",
    "        # set text_size for category names\n",
    "        text_size = rect_th / 3\n",
    "\n",
    "        cv2.rectangle(image, \n",
    "                      (x_min, y_min),\n",
    "                      (x_max, y_max), \n",
    "                      color=color,\n",
    "                      thickness=rect_th)\n",
    "        \n",
    "        p1, p2 = (int(x_min), int(y_min)), (int(x_max), int(y_max))\n",
    "        label = f\"{class_name}\"\n",
    "        w, h = cv2.getTextSize(label, 0, fontScale=text_size, thickness=text_th)[0]  # label width, height\n",
    "        outside = p1[1] - h - 3 >= 0  # label fits outside box\n",
    "        p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\n",
    "        \n",
    "        cv2.rectangle(image, p1, p2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            label,\n",
    "            (p1[0], p1[1] - 2 if outside else p1[1] + h + 2),\n",
    "            0,\n",
    "            text_size,\n",
    "            (255, 255, 255),\n",
    "            thickness=text_th,\n",
    "        )\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77a395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_label_files = sorted(glob(yolo_label_dir+\"/*.txt\"))\n",
    "infer_image_files = sorted(glob(yolo_image_dir+\"/*.jpg\"))\n",
    "\n",
    "@interact(index=(0,len(infer_image_files)))\n",
    "def verify_gt(index=0):\n",
    "    image = cv2.imread(infer_image_files[index])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    label = infer_label_files[index]\n",
    "    \n",
    "    bboxes = []\n",
    "    category_ids = []\n",
    "    \n",
    "    with open(label, 'r') as f:\n",
    "        line = f.readline().strip()\n",
    "        while line:\n",
    "            id, cx, cy, w, h = list(map(float, line.split()))\n",
    "            bboxes.append([cx, cy, w, h])\n",
    "            category_ids.append([int(id)])\n",
    "            line = f.readline().strip()\n",
    "\n",
    "    if len(bboxes) > 0:    \n",
    "        canvas = visualize_yolo(image, bboxes, category_ids)\n",
    "        plt.figure(figsize=(16,16))\n",
    "        plt.imshow(canvas)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf826fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efb3938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
