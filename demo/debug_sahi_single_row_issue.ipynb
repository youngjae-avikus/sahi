{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df174c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1771 이미지 이슈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd4ea5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from sahi import slicing\n",
    "from sahi.slicing import slice_image\n",
    "from sahi.predict import get_prediction, get_sliced_prediction, predict\n",
    "from sahi import AutoDetectionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e1ead27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fll_model_221014_path = '../resources/models/221014/best.pt'\n",
    "\n",
    "fll_model_221014 = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov5',\n",
    "    model_path=fll_model_221014_path,\n",
    "    confidence_threshold=0.25,\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "model = fll_model_221014\n",
    "model_path = fll_model_221014_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "978c3946",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image_dir = \"../resources/FLL_VAL/images/\"\n",
    "image_files = sorted([fn for fn in os.listdir(source_image_dir) if fn.endswith(\"jpg\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e88a8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_slice_mode=2\n",
    "custom_slice_x_start=640\n",
    "custom_slice_y_start=360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b75906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sahi.utils.cv import Colors\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def visualize_object_predictions(\n",
    "    image: np.array,\n",
    "    object_prediction_list,\n",
    "    rect_th: int = None,\n",
    "    text_size: float = None,\n",
    "    text_th: float = None,\n",
    "    color: tuple = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualizes prediction category names, bounding boxes over the source image\n",
    "    and exports it to output folder.\n",
    "    Arguments:\n",
    "        object_prediction_list: a list of prediction.ObjectPrediction\n",
    "        rect_th: rectangle thickness\n",
    "        text_size: size of the category name over box\n",
    "        text_th: text thickness\n",
    "        color: annotation color in the form: (0, 255, 0)\n",
    "        output_dir: directory for resulting visualization to be exported\n",
    "        file_name: exported file will be saved as: output_dir+file_name+\".png\"\n",
    "        export_format: can be specified as 'jpg' or 'png'\n",
    "    \"\"\"\n",
    "    # deepcopy image so that original is not altered\n",
    "    image = copy.deepcopy(image)\n",
    "    # select predefined classwise color palette if not specified\n",
    "    if color is None:\n",
    "        colors = Colors()\n",
    "    else:\n",
    "        colors = None\n",
    "    # set rect_th for boxes\n",
    "    rect_th = rect_th or max(round(sum(image.shape) / 2 * 0.001), 1)\n",
    "    # set text_th for category names\n",
    "    text_th = text_th or max(rect_th - 1, 1)\n",
    "    # set text_size for category names\n",
    "    text_size = text_size or rect_th / 3\n",
    "    # add bbox and mask to image if present\n",
    "    for object_prediction in object_prediction_list:\n",
    "        # deepcopy object_prediction_list so that original is not altered\n",
    "        object_prediction = object_prediction.deepcopy()\n",
    "\n",
    "        bbox = object_prediction.bbox.to_voc_bbox()\n",
    "        category_name = object_prediction.category.name\n",
    "        score = object_prediction.score.value\n",
    "\n",
    "        # set color\n",
    "        if colors is not None:\n",
    "            color = colors(object_prediction.category.id)\n",
    "        # visualize masks if present\n",
    "        if object_prediction.mask is not None:\n",
    "            # deepcopy mask so that original is not altered\n",
    "            mask = object_prediction.mask.bool_mask\n",
    "            # draw mask\n",
    "            rgb_mask = apply_color_mask(mask, color)\n",
    "            image = cv2.addWeighted(image, 1, rgb_mask, 0.4, 0)\n",
    "        # set bbox points\n",
    "        p1, p2 = (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3]))\n",
    "        # visualize boxes\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            p1,\n",
    "            p2,\n",
    "            color=color,\n",
    "            thickness=rect_th\n",
    "        )\n",
    "        # arange bounding box text location\n",
    "        label = f\"{category_name} {score:.2f}\"\n",
    "        w, h = cv2.getTextSize(label, 0, fontScale=text_size, thickness=text_th)[0]  # label width, height\n",
    "        outside = p1[1] - h - 3 >= 0  # label fits outside box\n",
    "        p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\n",
    "        # add bounding box text\n",
    "        cv2.rectangle(image, p1, p2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            label,\n",
    "            (p1[0], p1[1] - 2 if outside else p1[1] + h + 2),\n",
    "            0,\n",
    "            text_size,\n",
    "            (255, 255, 255),\n",
    "            thickness=text_th,\n",
    "        )\n",
    "        \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ce789f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sahi.prediction import PredictionResult\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from sahi.auto_model import AutoDetectionModel\n",
    "from sahi.model import DetectionModel\n",
    "from sahi.postprocess.combine import (\n",
    "    GreedyNMMPostprocess,\n",
    "    LSNMSPostprocess,\n",
    "    NMMPostprocess,\n",
    "    NMSPostprocess,\n",
    "    PostprocessPredictions,\n",
    ")\n",
    "from sahi.prediction import ObjectPrediction, PredictionResult\n",
    "from sahi.slicing import slice_image\n",
    "from sahi.utils.coco import Coco, CocoImage\n",
    "from sahi.utils.cv import (\n",
    "    IMAGE_EXTENSIONS,\n",
    "    VIDEO_EXTENSIONS,\n",
    "    crop_object_predictions,\n",
    "    cv2,\n",
    "    get_video_reader,\n",
    "    read_image_as_pil,\n",
    ")\n",
    "from sahi.utils.file import Path, increment_path, list_files, save_json, save_pickle\n",
    "from sahi.utils.import_utils import check_requirements\n",
    "\n",
    "POSTPROCESS_NAME_TO_CLASS = {\n",
    "    \"GREEDYNMM\": GreedyNMMPostprocess,\n",
    "    \"NMM\": NMMPostprocess,\n",
    "    \"NMS\": NMSPostprocess,\n",
    "    \"LSNMS\": LSNMSPostprocess,\n",
    "}\n",
    "\n",
    "def get_sliced_prediction(\n",
    "    image,\n",
    "    detection_model=None,\n",
    "    slice_height: int = None,\n",
    "    slice_width: int = None,\n",
    "    overlap_height_ratio: float = 0.2,\n",
    "    overlap_width_ratio: float = 0.2,\n",
    "    perform_standard_pred: bool = True,\n",
    "    postprocess_type: str = \"GREEDYNMM\",\n",
    "    postprocess_match_metric: str = \"IOS\",\n",
    "    postprocess_match_threshold: float = 0.5,\n",
    "    postprocess_class_agnostic: bool = False,\n",
    "    verbose: int = 1,\n",
    "    merge_buffer_length: int = None,\n",
    "    auto_slice_resolution: bool = True,\n",
    "    custom_slice_mode: int = 0,\n",
    "    custom_slice_x_start: int = 0,\n",
    "    custom_slice_y_start: int = 0,\n",
    ") -> PredictionResult:\n",
    "    \n",
    "    # for profiling\n",
    "    durations_in_seconds = dict()\n",
    "\n",
    "    # currently only 1 batch supported\n",
    "    num_batch = 1\n",
    "\n",
    "    # create slices from full image\n",
    "    time_start = time.time()\n",
    "    slice_image_result = slice_image(\n",
    "        image=image,\n",
    "        slice_height=slice_height,\n",
    "        slice_width=slice_width,\n",
    "        overlap_height_ratio=overlap_height_ratio,\n",
    "        overlap_width_ratio=overlap_width_ratio,\n",
    "        auto_slice_resolution=auto_slice_resolution,\n",
    "        custom_slice_mode=custom_slice_mode,\n",
    "        custom_slice_x_start=custom_slice_x_start,\n",
    "        custom_slice_y_start=custom_slice_y_start,\n",
    "    )\n",
    "    num_slices = len(slice_image_result)\n",
    "    time_end = time.time() - time_start\n",
    "    durations_in_seconds[\"slice\"] = time_end\n",
    "\n",
    "    # init match postprocess instance\n",
    "    if postprocess_type not in POSTPROCESS_NAME_TO_CLASS.keys():\n",
    "        raise ValueError(\n",
    "            f\"postprocess_type should be one of {list(POSTPROCESS_NAME_TO_CLASS.keys())} but given as {postprocess_type}\"\n",
    "        )\n",
    "    elif postprocess_type == \"UNIONMERGE\":\n",
    "        # deprecated in v0.9.3\n",
    "        raise ValueError(\"'UNIONMERGE' postprocess_type is deprecated, use 'GREEDYNMM' instead.\")\n",
    "    postprocess_constructor = POSTPROCESS_NAME_TO_CLASS[postprocess_type]\n",
    "    postprocess = postprocess_constructor(\n",
    "        match_threshold=postprocess_match_threshold,\n",
    "        match_metric=postprocess_match_metric,\n",
    "        class_agnostic=postprocess_class_agnostic,\n",
    "    )\n",
    "\n",
    "    # create prediction input\n",
    "    num_group = int(num_slices / num_batch)\n",
    "    if verbose == 1 or verbose == 2:\n",
    "        tqdm.write(f\"Performing prediction on {num_slices} number of slices.\")\n",
    "    object_prediction_list = []\n",
    "    # perform sliced prediction\n",
    "    for group_ind in range(num_group):\n",
    "        # prepare batch (currently supports only 1 batch)\n",
    "        image_list = []\n",
    "        shift_amount_list = []\n",
    "        for image_ind in range(num_batch):\n",
    "            image_list.append(slice_image_result.images[group_ind * num_batch + image_ind])\n",
    "            shift_amount_list.append(slice_image_result.starting_pixels[group_ind * num_batch + image_ind])\n",
    "        # perform batch prediction\n",
    "        prediction_result = get_prediction(\n",
    "            image=image_list[0],\n",
    "            detection_model=detection_model,\n",
    "            shift_amount=shift_amount_list[0],\n",
    "            full_shape=[\n",
    "                slice_image_result.original_image_height,\n",
    "                slice_image_result.original_image_width,\n",
    "            ],\n",
    "        )\n",
    "        # convert sliced predictions to full predictions\n",
    "        for object_prediction in prediction_result.object_prediction_list:\n",
    "            if object_prediction:  # if not empty\n",
    "                object_prediction_list.append(object_prediction.get_shifted_object_prediction())\n",
    "\n",
    "        # merge matching predictions during sliced prediction\n",
    "        if merge_buffer_length is not None and len(object_prediction_list) > merge_buffer_length:\n",
    "            object_prediction_list = postprocess(object_prediction_list)\n",
    "\n",
    "    # perform standard prediction\n",
    "    if num_slices > 0 and perform_standard_pred:\n",
    "        prediction_result = get_prediction(\n",
    "            image=image,\n",
    "            detection_model=detection_model,\n",
    "            shift_amount=[0, 0],\n",
    "            full_shape=None,\n",
    "            postprocess=None,\n",
    "        )\n",
    "        object_prediction_list.extend(prediction_result.object_prediction_list)\n",
    "    \n",
    "    # merge matching predictions\n",
    "    if len(object_prediction_list) > 1:\n",
    "        for object_prediction in object_prediction_list:\n",
    "            bbox = object_prediction.bbox.to_voc_bbox()\n",
    "            category_name = object_prediction.category.name\n",
    "            score = object_prediction.score.value\n",
    "            \n",
    "            print(bbox, category_name, score)\n",
    "        \n",
    "        object_prediction_list = postprocess(object_prediction_list)\n",
    "        \n",
    "        \n",
    "            \n",
    "    time_end = time.time() - time_start\n",
    "    durations_in_seconds[\"prediction\"] = time_end\n",
    "\n",
    "    if verbose == 2:\n",
    "        print(\n",
    "            \"Slicing performed in\",\n",
    "            durations_in_seconds[\"slice\"],\n",
    "            \"seconds.\",\n",
    "        )\n",
    "        print(\n",
    "            \"Prediction performed in\",\n",
    "            durations_in_seconds[\"prediction\"],\n",
    "            \"seconds.\",\n",
    "        )\n",
    "    \n",
    "    \n",
    "    return PredictionResult(\n",
    "        image=image, object_prediction_list=object_prediction_list, durations_in_seconds=durations_in_seconds\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76b8da59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2c25c7bd504114ac564416c72aff8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index', max=5905), IntSlider(value=512, description='sli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "@interact(index=(0, len(image_files)-1), slice_size=(0, 512), overlap_ratio=(0, 0.5, 0.05), only_full_inference=(0,1))\n",
    "def show_sample(index=0, slice_size=512, overlap_ratio=0.2, only_full_inference=0):\n",
    "    image_file = image_files[index]\n",
    "    image_path = os.path.join(source_image_dir, image_file)\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    if not only_full_inference:\n",
    "        slice_result = slice_image(image_path, \n",
    "                                  slice_width=slice_size,\n",
    "                                  slice_height=slice_size,\n",
    "                                  overlap_height_ratio=overlap_ratio,\n",
    "                                  overlap_width_ratio=overlap_ratio,\n",
    "                                  custom_slice_mode=custom_slice_mode,\n",
    "                                  custom_slice_x_start=custom_slice_x_start,\n",
    "                                  custom_slice_y_start=custom_slice_y_start,\n",
    "                                  verbose=1)\n",
    "\n",
    "        for start_pixel in slice_result.starting_pixels:\n",
    "            cv2.rectangle(image,\n",
    "                          start_pixel,\n",
    "                          [s1+s2 for s1, s2 in zip(start_pixel,[slice_size,slice_size])],\n",
    "                          color=(255, 255, 0),\n",
    "                          thickness=2)\n",
    "        \n",
    "        result = get_sliced_prediction(image_path,\n",
    "                                       model,\n",
    "                                       slice_height=slice_size,\n",
    "                                       slice_width=slice_size,\n",
    "                                       postprocess_match_threshold=0.5,\n",
    "                                       overlap_height_ratio=overlap_ratio,\n",
    "                                       overlap_width_ratio=overlap_ratio,\n",
    "                                       custom_slice_mode=custom_slice_mode,\n",
    "                                       postprocess_type=\"GREEDYNMM\",\n",
    "                                       custom_slice_x_start=custom_slice_x_start,\n",
    "                                       custom_slice_y_start=custom_slice_y_start\n",
    "                                      )\n",
    "    else:\n",
    "        result = get_prediction(image_path, model)\n",
    "    \n",
    "    canvas = visualize_object_predictions(image, result.object_prediction_list)\n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.imshow(canvas)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbbaa96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2a233d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1698a152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from sahi import slicing\n",
    "from sahi.slicing import slice_image\n",
    "from sahi.predict import get_prediction, get_sliced_prediction, predict\n",
    "from sahi import AutoDetectionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd16cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image_dir = \"../resources/FLL_VAL/images/\"\n",
    "image_files = sorted([fn for fn in os.listdir(source_image_dir) if fn.endswith(\"jpg\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6473038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fll_model_221012_path = '../resources/models/221012/best.pt'\n",
    "\n",
    "fll_model_221012 = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov5',\n",
    "    model_path=fll_model_221012_path,\n",
    "    confidence_threshold=0.25,\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "model = fll_model_221012\n",
    "model_path = fll_model_221012_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea21e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sahi.utils.cv import Colors\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def visualize_object_predictions(\n",
    "    image: np.array,\n",
    "    object_prediction_list,\n",
    "    rect_th: int = None,\n",
    "    text_size: float = None,\n",
    "    text_th: float = None,\n",
    "    color: tuple = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualizes prediction category names, bounding boxes over the source image\n",
    "    and exports it to output folder.\n",
    "    Arguments:\n",
    "        object_prediction_list: a list of prediction.ObjectPrediction\n",
    "        rect_th: rectangle thickness\n",
    "        text_size: size of the category name over box\n",
    "        text_th: text thickness\n",
    "        color: annotation color in the form: (0, 255, 0)\n",
    "        output_dir: directory for resulting visualization to be exported\n",
    "        file_name: exported file will be saved as: output_dir+file_name+\".png\"\n",
    "        export_format: can be specified as 'jpg' or 'png'\n",
    "    \"\"\"\n",
    "    # deepcopy image so that original is not altered\n",
    "    image = copy.deepcopy(image)\n",
    "    # select predefined classwise color palette if not specified\n",
    "    if color is None:\n",
    "        colors = Colors()\n",
    "    else:\n",
    "        colors = None\n",
    "    # set rect_th for boxes\n",
    "    rect_th = rect_th or max(round(sum(image.shape) / 2 * 0.001), 1)\n",
    "    # set text_th for category names\n",
    "    text_th = text_th or max(rect_th - 1, 1)\n",
    "    # set text_size for category names\n",
    "    text_size = text_size or rect_th / 3\n",
    "    # add bbox and mask to image if present\n",
    "    for object_prediction in object_prediction_list:\n",
    "        # deepcopy object_prediction_list so that original is not altered\n",
    "        object_prediction = object_prediction.deepcopy()\n",
    "\n",
    "        bbox = object_prediction.bbox.to_voc_bbox()\n",
    "        category_name = object_prediction.category.name\n",
    "        score = object_prediction.score.value\n",
    "\n",
    "        # set color\n",
    "        if colors is not None:\n",
    "            color = colors(object_prediction.category.id)\n",
    "        # visualize masks if present\n",
    "        if object_prediction.mask is not None:\n",
    "            # deepcopy mask so that original is not altered\n",
    "            mask = object_prediction.mask.bool_mask\n",
    "            # draw mask\n",
    "            rgb_mask = apply_color_mask(mask, color)\n",
    "            image = cv2.addWeighted(image, 1, rgb_mask, 0.4, 0)\n",
    "        # set bbox points\n",
    "        p1, p2 = (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3]))\n",
    "        # visualize boxes\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            p1,\n",
    "            p2,\n",
    "            color=color,\n",
    "            thickness=rect_th\n",
    "        )\n",
    "        # arange bounding box text location\n",
    "        label = f\"{category_name} {score:.2f}\"\n",
    "        w, h = cv2.getTextSize(label, 0, fontScale=text_size, thickness=text_th)[0]  # label width, height\n",
    "        outside = p1[1] - h - 3 >= 0  # label fits outside box\n",
    "        p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\n",
    "        # add bounding box text\n",
    "        cv2.rectangle(image, p1, p2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            label,\n",
    "            (p1[0], p1[1] - 2 if outside else p1[1] + h + 2),\n",
    "            0,\n",
    "            text_size,\n",
    "            (255, 255, 255),\n",
    "            thickness=text_th,\n",
    "        )\n",
    "        \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a87df05",
   "metadata": {},
   "source": [
    "## Draw Slice Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc1b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "%matplotlib inline \n",
    "from sahi import slicing\n",
    "from sahi.slicing import slice_image\n",
    "\n",
    "slicing.logger.setLevel(slicing.logging.INFO)\n",
    "\n",
    "# single_row_y_start: int = 200,\n",
    "@interact(index=(0, len(image_files)-1), slice_size=(0, 512), overlap_ratio=(0, 0.5, 0.05), single_row_y_start=(0, 512))\n",
    "def visualize_slice_rect(index=0, slice_size=512, overlap_ratio=0.2, single_row_y_start=200):\n",
    "    image_file = image_files[index]\n",
    "    image_path = os.path.join(source_image_dir, image_file)\n",
    "    \n",
    "    res = slice_image(image_path, \n",
    "                      slice_width=slice_size,\n",
    "                      slice_height=slice_size,\n",
    "                      overlap_height_ratio=overlap_ratio,\n",
    "                      overlap_width_ratio=overlap_ratio,\n",
    "                      single_row_y_start=single_row_y_start,\n",
    "                      single_row_predict=True,\n",
    "                      verbose=1)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=len(res.images)//2+1,ncols=2,figsize=(12,16))\n",
    "    \n",
    "#     plt.subplots_adjust(left=0.05, bottom=0.01, right=0.99, \n",
    "#                     top=0.99, wspace=None, hspace=0.2)\n",
    "    \n",
    "    ax = axes.flatten()\n",
    "    \n",
    "    for img_idx, s_im in enumerate(res.images, 0):\n",
    "        ax[img_idx].imshow(s_im)\n",
    "        ax[img_idx].axis('off')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81ff1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "@interact(index=(0, len(image_files)-1), slice_size=(0, 512), overlap_ratio=(0, 0.5, 0.05), single_row_y_start=(0, 512), only_full_inference=(0,1))\n",
    "def show_sample(index=0, slice_size=512, overlap_ratio=0.2, single_row_y_start=200, only_full_inference=0):\n",
    "    image_file = image_files[index]\n",
    "    image_path = os.path.join(source_image_dir, image_file)\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    if not only_full_inference:\n",
    "        slice_result = slice_image(image_path, \n",
    "                                  slice_width=slice_size,\n",
    "                                  slice_height=slice_size,\n",
    "                                  overlap_height_ratio=overlap_ratio,\n",
    "                                  overlap_width_ratio=overlap_ratio,\n",
    "                                  single_row_y_start=single_row_y_start,\n",
    "                                  single_row_predict=True,\n",
    "                                  verbose=1)\n",
    "\n",
    "        for start_pixel in slice_result.starting_pixels:\n",
    "            cv2.rectangle(image,\n",
    "                          start_pixel,\n",
    "                          [s1+s2 for s1, s2 in zip(start_pixel,[slice_size,slice_size])],\n",
    "                          color=(255, 255, 0),\n",
    "                          thickness=2)\n",
    "        \n",
    "        result = get_sliced_prediction(image_path,\n",
    "                                       model,\n",
    "                                       slice_height=slice_size,\n",
    "                                       slice_width=slice_size,\n",
    "                                       postprocess_match_threshold=0.5,\n",
    "                                       overlap_height_ratio=overlap_ratio,\n",
    "                                       overlap_width_ratio=overlap_ratio,\n",
    "                                       single_row_y_start=single_row_y_start,\n",
    "                                       single_row_predict=True)\n",
    "    else:\n",
    "        result = get_prediction(image_path, model)\n",
    "    \n",
    "    canvas = visualize_object_predictions(image, result.object_prediction_list)\n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.imshow(canvas)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b757ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1771"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b15bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4852ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb8de8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
