{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f461dc42",
   "metadata": {},
   "source": [
    "# SAHI Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b513e",
   "metadata": {},
   "source": [
    "## 1. FULL INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71949b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 24 01:06:59 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA Graphics...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   63C    P3     6W /  N/A |   1912MiB /  3913MiB |      3%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1147      G   /usr/lib/xorg/Xorg                 44MiB |\r\n",
      "|    0   N/A  N/A      1403      G   /usr/bin/gnome-shell                5MiB |\r\n",
      "|    0   N/A  N/A      1867      G   /usr/lib/xorg/Xorg                325MiB |\r\n",
      "|    0   N/A  N/A      2040      G   /usr/bin/gnome-shell              189MiB |\r\n",
      "|    0   N/A  N/A      3991      G   ...818280098758357408,131072      348MiB |\r\n",
      "|    0   N/A  N/A      7125    C+G   /usr/bin/python3                  884MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8efb89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrange an instance segmentation model for test\n",
    "from sahi.utils.yolov5 import (\n",
    "    download_yolov5s6_model,\n",
    ")\n",
    "\n",
    "# import required functions, classes\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.utils.cv import read_image\n",
    "from sahi.utils.file import download_from_url\n",
    "from sahi.predict import get_prediction, get_sliced_prediction, predict\n",
    "from sahi.scripts.coco_error_analysis import analyse\n",
    "from sahi.scripts.coco_evaluation import evaluate\n",
    "from IPython.display import Image\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f251a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image_dir = \"../resources/FLL_VAL/images/\"\n",
    "source_label_dir = \"../resources/FLL_VAL/labels/\"\n",
    "\n",
    "coco_m_path = '../resources/models/yolov5m.pt'\n",
    "fll_model_221007_path = '../resources/models/221007/best.pt'\n",
    "fll_model_221012_path = '../resources/models/221012/best.pt'\n",
    "fll_model_221013_path = '../resources/models/221013/best.pt'\n",
    "fll_model_221014_path = '../resources/models/221014/best.pt'\n",
    "fll_model_221021_path = '../resources/models/221021/best.pt'\n",
    "fll_model_221021_960_path = '../resources/models/221021_960/best.pt'\n",
    "fll_model_221022_960_path = '../resources/models/221022_960/best.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa69f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fll_model_221014 = AutoDetectionModel.from_pretrained(\n",
    "#     model_type='yolov5',\n",
    "#     model_path=fll_model_221014_path,\n",
    "#     confidence_threshold=0.25,\n",
    "#     device=\"cuda:0\"\n",
    "# )\n",
    "\n",
    "# fll_model_221021_960 = AutoDetectionModel.from_pretrained(\n",
    "#     model_type='yolov5',\n",
    "#     model_path=fll_model_221021_960_path,\n",
    "#     confidence_threshold=0.25,\n",
    "#     device=\"cuda:0\"\n",
    "# )\n",
    "\n",
    "fll_model_221022_960 = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov5',\n",
    "    model_path=fll_model_221022_960_path,\n",
    "    confidence_threshold=0.25,\n",
    "    device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caa71aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fll_model_221022_960\n",
    "model_path = fll_model_221022_960_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3e9b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_extract(img_dir, label_dir, out_dir):\n",
    "    if os.path.exists(os.path.join(out_dir, 'val.json')):\n",
    "        os.remove(os.path.join(out_dir, 'val.json'))\n",
    "    \n",
    "    licenses = [\n",
    "        {\n",
    "            \"name\": \"\",\n",
    "            \"id\": 0,\n",
    "            \"url\": \"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    info_ = [\n",
    "        {\n",
    "            \"contributor\": \"\",\n",
    "            \"date_created\": \"\",\n",
    "            \"description\": \"\",\n",
    "            \"url\": \"\",\n",
    "            \"version\": \"\",\n",
    "            \"year\": \"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    categories = [\n",
    "        {\n",
    "            \"id\": 0,\n",
    "            \"name\": \"Buoy\",\n",
    "            \"supercategory\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"name\": \"Boat\",\n",
    "            \"supercategory\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"name\": \"Channel Marker\",\n",
    "            \"supercategory\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 3,\n",
    "            \"name\": \"Speed Warning Sign\",\n",
    "            \"supercategory\": \"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    img_idx = 0\n",
    "    annot_idx = 0\n",
    "\n",
    "    imgs_list = []\n",
    "    annots_list = []\n",
    "\n",
    "    for label_file in sorted(os.listdir(label_dir)):\n",
    "        label_file_ = os.path.join(label_dir, label_file)\n",
    "        img_file_ = os.path.join(img_dir, f'{os.path.splitext(label_file)[0]}.jpg')\n",
    "        img = Image.open(img_file_)\n",
    "        image_w, image_h = img.size\n",
    "\n",
    "        imgs_list.append({\n",
    "            'id': img_idx,\n",
    "            'width': image_w,\n",
    "            'height': image_h,\n",
    "            'file_name': f'{os.path.splitext(label_file)[0]}.jpg',\n",
    "            \"license\": 0,\n",
    "            \"flickr_url\": \"\",\n",
    "            \"coco_url\": \"\",\n",
    "            \"date_captured\": 0\n",
    "        })\n",
    "\n",
    "        with open(label_file_, 'r') as label_f:\n",
    "            labels = label_f.readlines()\n",
    "\n",
    "            for label in labels:\n",
    "                cat, xc, yc, label_normalized_w, label_normalized_h = list(map(lambda x: int(x) if len(x) == 1 else float(x), label.split()))\n",
    "                label_w, label_h = image_w * label_normalized_w, image_h * label_normalized_h\n",
    "                xmin, ymin = (image_w * xc) - (label_w / 2), (image_h * yc) - (label_h / 2)\n",
    "                \n",
    "                xmin = 0 if xmin < 0 else xmin\n",
    "                ymin = 0 if ymin < 0 else ymin\n",
    "\n",
    "                annots_list.append({\n",
    "                    'id': annot_idx,\n",
    "                    'image_id': img_idx,\n",
    "                    'category_id': cat,\n",
    "                    'area': int(label_h * label_w),\n",
    "                    'bbox': [\n",
    "                        xmin,\n",
    "                        ymin,\n",
    "                        label_w,\n",
    "                        label_h\n",
    "                    ],\n",
    "                    'iscrowd': 0,\n",
    "                    'attributes': {\n",
    "                        'type': '',\n",
    "                        'occluded': False\n",
    "                    },\n",
    "                    'segmentation': []\n",
    "                })\n",
    "\n",
    "                annot_idx += 1\n",
    "\n",
    "        img_idx += 1\n",
    "\n",
    "    out_dict = {\n",
    "        'licenses': licenses,\n",
    "        'info': info_,\n",
    "        'categories': categories,\n",
    "        'images': imgs_list,\n",
    "        'annotations': annots_list\n",
    "    }\n",
    "    \n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(out_dir, 'val.json'), 'w') as out_f:\n",
    "        print(os.path.join(out_dir, 'val.json'))\n",
    "        json.dump(out_dict, out_f)\n",
    "        \n",
    "    return os.path.join(out_dir, 'val.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8de067ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../resources/FLL_VAL/val.json\n"
     ]
    }
   ],
   "source": [
    "# initial_extract(img_dir, label_dir, out_dir)\n",
    "gt_json_path = initial_extract(source_image_dir, source_label_dir, str(Path(source_image_dir).parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bd4e997",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"yolov5\"\n",
    "MODEL_PATH = model_path\n",
    "MODEL_CONFIG_PATH = \"\"\n",
    "EVAL_IMAGES_FOLDER_DIR = source_image_dir\n",
    "EVAL_DATASET_JSON_PATH = gt_json_path\n",
    "INFERENCE_SETTING = \"AVIKUS_FL\"\n",
    "EXPORT_VISUAL = False\n",
    "MAX_DETECTIONS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58d67035",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_SETTING_TO_PARAMS = {\n",
    "    \"AVIKUS_FL\": {\n",
    "        \"no_standard_prediction\": False,\n",
    "        \"no_sliced_prediction\": True,\n",
    "        \"slice_size\": 512,\n",
    "        \"overlap_ratio\": 0.15,\n",
    "        \"match_threshold\": 0.5,\n",
    "        \"postprocess_class_agnostic\": False,\n",
    "        \"custom_slice_y_start\": 200,\n",
    "    },\n",
    "}\n",
    "\n",
    "setting_params = INFERENCE_SETTING_TO_PARAMS[INFERENCE_SETTING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "085fba4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexing coco dataset annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading coco annotations: 100%|███████████████████████| 4897/4897 [00:01<00:00, 4214.97it/s]\n",
      "Performing inference on images:   0%|                      | 2/4897 [00:00<06:14, 13.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time is: 68.28 ms\n",
      "Prediction time is: 58.25 ms\n",
      "Prediction time is: 59.46 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing inference on images:   0%|                      | 6/4897 [00:00<05:52, 13.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time is: 59.96 ms\n",
      "Prediction time is: 56.58 ms\n",
      "Prediction time is: 56.84 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing inference on images:   0%|                      | 8/4897 [00:00<05:50, 13.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time is: 58.79 ms\n",
      "Prediction time is: 60.46 ms\n",
      "Prediction time is: 60.32 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing inference on images:   0%|                     | 12/4897 [00:00<05:55, 13.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time is: 58.15 ms\n",
      "Prediction time is: 61.07 ms\n",
      "Prediction time is: 55.42 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing inference on images:   0%|                     | 14/4897 [00:01<05:50, 13.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time is: 56.76 ms\n",
      "Prediction time is: 56.73 ms\n",
      "Prediction time is: 65.93 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing inference on images:   0%|                     | 18/4897 [00:01<06:01, 13.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time is: 56.72 ms\n",
      "Prediction time is: 60.45 ms\n",
      "Prediction time is: 61.46 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing inference on images:   0%|                     | 20/4897 [00:01<05:59, 13.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time is: 61.65 ms\n",
      "Prediction time is: 57.71 ms\n",
      "Prediction time is: 62.86 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing inference on images:   0%|                     | 24/4897 [00:01<05:58, 13.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time is: 64.09 ms\n",
      "Prediction time is: 59.43 ms\n",
      "Prediction time is: 55.20 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing inference on images:   1%|                     | 26/4897 [00:01<05:54, 13.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time is: 57.12 ms\n",
      "Prediction time is: 59.31 ms\n",
      "Prediction time is: 60.36 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing inference on images:   1%|▏                    | 30/4897 [00:02<05:57, 13.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time is: 58.67 ms\n",
      "Prediction time is: 61.39 ms\n",
      "Prediction time is: 62.90 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing inference on images:   1%|▏                    | 31/4897 [00:02<06:08, 13.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time is: 61.75 ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_TYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_CONFIG_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_confidence_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_category_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_category_remapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEVAL_IMAGES_FOLDER_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_standard_prediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msetting_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno_standard_prediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_sliced_prediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msetting_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno_sliced_prediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mslice_height\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msetting_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslice_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mslice_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msetting_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslice_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverlap_height_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msetting_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverlap_ratio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverlap_width_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msetting_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverlap_ratio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m960\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpostprocess_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGREEDYNMM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpostprocess_match_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIOS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpostprocess_match_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msetting_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatch_threshold\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpostprocess_class_agnostic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msetting_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpostprocess_class_agnostic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnovisual\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mEXPORT_VISUAL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_json_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEVAL_DATASET_JSON_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mruns/FLL_221022_960_0.25_STANDARD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINFERENCE_SETTING\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisual_bbox_thickness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisual_text_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisual_text_thickness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisual_export_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_postprocess_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_slice_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_slice_x_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_slice_y_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m result_json_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(Path(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexport_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/sahi/lib/python3.8/site-packages/sahi-0.10.7-py3.8.egg/sahi/predict.py:533\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(detection_model, model_type, model_path, model_config_path, model_confidence_threshold, model_device, model_category_mapping, model_category_remapping, source, no_standard_prediction, no_sliced_prediction, image_size, slice_height, slice_width, overlap_height_ratio, overlap_width_ratio, postprocess_type, postprocess_match_metric, postprocess_match_threshold, postprocess_class_agnostic, novisual, view_video, frame_skip_interval, export_pickle, export_crop, dataset_json_path, project, name, visual_bbox_thickness, visual_text_size, visual_text_thickness, visual_export_format, verbose, return_dict, force_postprocess_type, custom_slice_mode, custom_slice_x_start, custom_slice_y_start)\u001b[0m\n\u001b[1;32m    530\u001b[0m     durations_in_seconds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslice\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prediction_result\u001b[38;5;241m.\u001b[39mdurations_in_seconds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslice\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# get standard prediction\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     prediction_result \u001b[38;5;241m=\u001b[39m \u001b[43mget_prediction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_as_pil\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetection_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetection_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshift_amount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpostprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m     object_prediction_list \u001b[38;5;241m=\u001b[39m prediction_result\u001b[38;5;241m.\u001b[39mobject_prediction_list\n\u001b[1;32m    543\u001b[0m durations_in_seconds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prediction_result\u001b[38;5;241m.\u001b[39mdurations_in_seconds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/sahi/lib/python3.8/site-packages/sahi-0.10.7-py3.8.egg/sahi/predict.py:91\u001b[0m, in \u001b[0;36mget_prediction\u001b[0;34m(image, detection_model, shift_amount, full_shape, postprocess, verbose)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# get prediction\u001b[39;00m\n\u001b[1;32m     90\u001b[0m time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 91\u001b[0m \u001b[43mdetection_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_as_pil\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m time_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m time_start\n\u001b[1;32m     93\u001b[0m durations_in_seconds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m time_end\n",
      "File \u001b[0;32m~/anaconda3/envs/sahi/lib/python3.8/site-packages/sahi-0.10.7-py3.8.egg/sahi/model.py:416\u001b[0m, in \u001b[0;36mYolov5DetectionModel.perform_inference\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel is not loaded, load it by calling .load_model()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     prediction_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     prediction_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(image)\n",
      "File \u001b[0;32m~/anaconda3/envs/sahi/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/anaconda3/envs/sahi/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sahi/lib/python3.8/site-packages/yolov5-6.2.1-py3.8.egg/yolov5/models/common.py:679\u001b[0m, in \u001b[0;36mAutoShape.forward\u001b[0;34m(self, ims, size, augment, profile, nms)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mautocast(autocast):\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dt[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 679\u001b[0m         y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x, augment, profile)  \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;66;03m# Post-process\u001b[39;00m\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dt[\u001b[38;5;241m2\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/sahi/lib/python3.8/site-packages/yolov5-6.2.1-py3.8.egg/yolov5/utils/general.py:155\u001b[0m, in \u001b[0;36mProfile.__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m, value, traceback):\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart  \u001b[38;5;66;03m# delta-time\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt\n",
      "File \u001b[0;32m~/anaconda3/envs/sahi/lib/python3.8/site-packages/yolov5-6.2.1-py3.8.egg/yolov5/utils/general.py:160\u001b[0m, in \u001b[0;36mProfile.time\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtime\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda:\n\u001b[0;32m--> 160\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/anaconda3/envs/sahi/lib/python3.8/site-packages/torch/cuda/__init__.py:402\u001b[0m, in \u001b[0;36msynchronize\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    400\u001b[0m _lazy_init()\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[0;32m--> 402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_synchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = predict(\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_path=MODEL_PATH,\n",
    "    model_config_path=MODEL_CONFIG_PATH,\n",
    "    model_confidence_threshold=0.25,\n",
    "    model_device=\"cuda:0\",\n",
    "    model_category_mapping=None,\n",
    "    model_category_remapping=None,\n",
    "    source=EVAL_IMAGES_FOLDER_DIR,\n",
    "    no_standard_prediction=setting_params[\"no_standard_prediction\"],\n",
    "    no_sliced_prediction=setting_params[\"no_sliced_prediction\"],\n",
    "    slice_height=setting_params[\"slice_size\"],\n",
    "    slice_width=setting_params[\"slice_size\"],\n",
    "    overlap_height_ratio=setting_params[\"overlap_ratio\"],\n",
    "    overlap_width_ratio=setting_params[\"overlap_ratio\"],\n",
    "    image_size=960,\n",
    "    postprocess_type=\"GREEDYNMM\",\n",
    "    postprocess_match_metric=\"IOS\",\n",
    "    postprocess_match_threshold=setting_params[\"match_threshold\"],\n",
    "    postprocess_class_agnostic=setting_params[\"postprocess_class_agnostic\"],\n",
    "    novisual=not EXPORT_VISUAL,\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    project=\"runs/FLL_221022_960_0.25_STANDARD\",\n",
    "    name=INFERENCE_SETTING,\n",
    "    visual_bbox_thickness=None,\n",
    "    visual_text_size=None,\n",
    "    visual_text_thickness=None,\n",
    "    visual_export_format=\"png\",\n",
    "    verbose=2,\n",
    "    return_dict=True,\n",
    "    force_postprocess_type=True,\n",
    "    custom_slice_mode=0,\n",
    "    custom_slice_x_start=0,\n",
    "    custom_slice_y_start=0,\n",
    ")\n",
    "\n",
    "result_json_path = str(Path(result[\"export_dir\"]) / \"result.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dict = evaluate(\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    result_json_path=result_json_path,\n",
    "    classwise=True,\n",
    "    max_detections=MAX_DETECTIONS,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c844485",
   "metadata": {},
   "source": [
    "## Mode 2 (One Box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab7483",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_SETTING_TO_PARAMS = {\n",
    "    \"AVIKUS_FL\": {\n",
    "        \"no_standard_prediction\": False,\n",
    "        \"no_sliced_prediction\": False,\n",
    "        \"slice_size\": 512,\n",
    "        \"overlap_ratio\": 0.15,\n",
    "        \"match_threshold\": 0.5,\n",
    "        \"postprocess_class_agnostic\": False,\n",
    "        \"custom_slice_x_start\": 640,\n",
    "        \"custom_slice_y_start\": 360,\n",
    "        \"custom_slice_mode\": 2\n",
    "    },\n",
    "}\n",
    "\n",
    "setting_params = INFERENCE_SETTING_TO_PARAMS[INFERENCE_SETTING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93797401",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_path=MODEL_PATH,\n",
    "    model_config_path=MODEL_CONFIG_PATH,\n",
    "    model_confidence_threshold=0.25,\n",
    "    model_device=\"cuda:0\",\n",
    "    model_category_mapping=None,\n",
    "    model_category_remapping=None,\n",
    "    source=EVAL_IMAGES_FOLDER_DIR,\n",
    "    no_standard_prediction=setting_params[\"no_standard_prediction\"],\n",
    "    no_sliced_prediction=setting_params[\"no_sliced_prediction\"],\n",
    "    slice_height=setting_params[\"slice_size\"],\n",
    "    slice_width=setting_params[\"slice_size\"],\n",
    "    overlap_height_ratio=setting_params[\"overlap_ratio\"],\n",
    "    overlap_width_ratio=setting_params[\"overlap_ratio\"],\n",
    "    postprocess_type=\"GREEDYNMM\",\n",
    "    postprocess_match_metric=\"IOS\",\n",
    "    postprocess_match_threshold=setting_params[\"match_threshold\"],\n",
    "    postprocess_class_agnostic=setting_params[\"postprocess_class_agnostic\"],\n",
    "    novisual=not EXPORT_VISUAL,\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    project=\"runs/mode0\",\n",
    "    name=INFERENCE_SETTING,\n",
    "    visual_bbox_thickness=None,\n",
    "    visual_text_size=None,\n",
    "    visual_text_thickness=None,\n",
    "    visual_export_format=\"png\",\n",
    "    verbose=2,\n",
    "    return_dict=True,\n",
    "    force_postprocess_type=True,\n",
    "    custom_slice_mode=setting_params[\"custom_slice_mode\"],\n",
    "    custom_slice_x_start=setting_params[\"custom_slice_x_start\"],\n",
    "    custom_slice_y_start=setting_params[\"custom_slice_y_start\"],\n",
    ")\n",
    "\n",
    "result_json_path = str(Path(result[\"export_dir\"]) / \"result.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036d7643",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dict = evaluate(\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    result_json_path=result_json_path,\n",
    "    classwise=True,\n",
    "    max_detections=MAX_DETECTIONS,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc75328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_path=MODEL_PATH,\n",
    "    model_config_path=MODEL_CONFIG_PATH,\n",
    "    model_confidence_threshold=0.25,\n",
    "    model_device=\"cuda:0\",\n",
    "    model_category_mapping=None,\n",
    "    model_category_remapping=None,\n",
    "    source=EVAL_IMAGES_FOLDER_DIR,\n",
    "    no_standard_prediction=setting_params[\"no_standard_prediction\"],\n",
    "    no_sliced_prediction=setting_params[\"no_sliced_prediction\"],\n",
    "    slice_height=setting_params[\"slice_size\"],\n",
    "    slice_width=setting_params[\"slice_size\"],\n",
    "    overlap_height_ratio=setting_params[\"overlap_ratio\"],\n",
    "    overlap_width_ratio=setting_params[\"overlap_ratio\"],\n",
    "    postprocess_type=\"NMM\",\n",
    "    postprocess_match_metric=\"IOS\",\n",
    "    postprocess_match_threshold=setting_params[\"match_threshold\"],\n",
    "    postprocess_class_agnostic=setting_params[\"postprocess_class_agnostic\"],\n",
    "    novisual=not EXPORT_VISUAL,\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    project=\"runs/mode0\",\n",
    "    name=INFERENCE_SETTING,\n",
    "    visual_bbox_thickness=None,\n",
    "    visual_text_size=None,\n",
    "    visual_text_thickness=None,\n",
    "    visual_export_format=\"png\",\n",
    "    verbose=2,\n",
    "    return_dict=True,\n",
    "    force_postprocess_type=True,\n",
    "    custom_slice_mode=setting_params[\"custom_slice_mode\"],\n",
    "    custom_slice_x_start=setting_params[\"custom_slice_x_start\"],\n",
    "    custom_slice_y_start=setting_params[\"custom_slice_y_start\"],\n",
    ")\n",
    "\n",
    "result_json_path = str(Path(result[\"export_dir\"]) / \"result.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bab3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dict = evaluate(\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    result_json_path=result_json_path,\n",
    "    classwise=True,\n",
    "    max_detections=MAX_DETECTIONS,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94e3f82",
   "metadata": {},
   "source": [
    "## Mode 3 (Four Box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7663ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_SETTING_TO_PARAMS = {\n",
    "    \"AVIKUS_FL\": {\n",
    "        \"no_standard_prediction\": False,\n",
    "        \"no_sliced_prediction\": False,\n",
    "        \"slice_size\": 360,\n",
    "        \"overlap_ratio\": 0.15,\n",
    "        \"match_threshold\": 0.5,\n",
    "        \"postprocess_class_agnostic\": False,\n",
    "        \"custom_slice_x_start\": 580,\n",
    "        \"custom_slice_y_start\": 280,\n",
    "        \"custom_slice_mode\": 3\n",
    "    },\n",
    "}\n",
    "\n",
    "setting_params = INFERENCE_SETTING_TO_PARAMS[INFERENCE_SETTING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd0cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_path=MODEL_PATH,\n",
    "    model_config_path=MODEL_CONFIG_PATH,\n",
    "    model_confidence_threshold=0.001,\n",
    "    model_device=\"cuda:0\",\n",
    "    model_category_mapping=None,\n",
    "    model_category_remapping=None,\n",
    "    source=EVAL_IMAGES_FOLDER_DIR,\n",
    "    no_standard_prediction=setting_params[\"no_standard_prediction\"],\n",
    "    no_sliced_prediction=setting_params[\"no_sliced_prediction\"],\n",
    "    slice_height=setting_params[\"slice_size\"],\n",
    "    slice_width=setting_params[\"slice_size\"],\n",
    "    overlap_height_ratio=setting_params[\"overlap_ratio\"],\n",
    "    overlap_width_ratio=setting_params[\"overlap_ratio\"],\n",
    "    postprocess_type=\"GREEDYNMM\",\n",
    "    postprocess_match_metric=\"IOS\",\n",
    "    postprocess_match_threshold=setting_params[\"match_threshold\"],\n",
    "    postprocess_class_agnostic=setting_params[\"postprocess_class_agnostic\"],\n",
    "    novisual=not EXPORT_VISUAL,\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    project=\"runs/mode3\",\n",
    "    name=INFERENCE_SETTING,\n",
    "    visual_bbox_thickness=None,\n",
    "    visual_text_size=None,\n",
    "    visual_text_thickness=None,\n",
    "    visual_export_format=\"png\",\n",
    "    verbose=2,\n",
    "    return_dict=True,\n",
    "    force_postprocess_type=True,\n",
    "    custom_slice_mode=setting_params[\"custom_slice_mode\"],\n",
    "    custom_slice_x_start=setting_params[\"custom_slice_x_start\"],\n",
    "    custom_slice_y_start=setting_params[\"custom_slice_y_start\"],\n",
    ")\n",
    "\n",
    "result_json_path = str(Path(result[\"export_dir\"]) / \"result.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb2152",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dict = evaluate(\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    result_json_path=result_json_path,\n",
    "    classwise=True,\n",
    "    max_detections=MAX_DETECTIONS,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9eea1e",
   "metadata": {},
   "source": [
    "## DSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c22b6579",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsl_result_json = \"../../AiBoat/aiboat/APP/NAS/deepstream-services-library/examples/python/result_FL221022_0.25_standard_pred.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "947afd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.12s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=2.72s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.18s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.241\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=300 ] = 0.471\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=300 ] = 0.213\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=300 ] = 0.393\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=300 ] = 0.721\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=300 ] = 0.688\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=300 ] = 0.167\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=300 ] = 0.403\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=300 ] = 0.495\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=300 ] = 0.221\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=300 ] = 0.473\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=300 ] = 0.520\n",
      " Average Precision  (AP) Buoy               @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.015\n",
      " Average Precision  (AP) Buoy               @[ IoU=0.50:0.95 | area= small | maxDets=300 ] = 0.017\n",
      " Average Precision  (AP) Buoy               @[ IoU=0.50:0.95 | area=medium | maxDets=300 ] = -1.000\n",
      " Average Precision  (AP) Buoy               @[ IoU=0.50:0.95 | area= large | maxDets=300 ] = -1.000\n",
      " Average Precision  (AP) Buoy               @[ IoU=0.50      | area=   all | maxDets=300 ] = 0.054\n",
      " Average Precision  (AP) Buoy               @[ IoU=0.50      | area= small | maxDets=300 ] = 0.058\n",
      " Average Precision  (AP) Buoy               @[ IoU=0.50      | area=medium | maxDets=300 ] = -1.000\n",
      " Average Precision  (AP) Buoy               @[ IoU=0.50      | area= large | maxDets=300 ] = -1.000\n",
      " Average Precision  (AP) Boat               @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.511\n",
      " Average Precision  (AP) Boat               @[ IoU=0.50:0.95 | area= small | maxDets=300 ] = 0.302\n",
      " Average Precision  (AP) Boat               @[ IoU=0.50:0.95 | area=medium | maxDets=300 ] = 0.495\n",
      " Average Precision  (AP) Boat               @[ IoU=0.50:0.95 | area= large | maxDets=300 ] = 0.640\n",
      " Average Precision  (AP) Boat               @[ IoU=0.50      | area=   all | maxDets=300 ] = 0.845\n",
      " Average Precision  (AP) Boat               @[ IoU=0.50      | area= small | maxDets=300 ] = 0.630\n",
      " Average Precision  (AP) Boat               @[ IoU=0.50      | area=medium | maxDets=300 ] = 0.847\n",
      " Average Precision  (AP) Boat               @[ IoU=0.50      | area= large | maxDets=300 ] = 0.924\n",
      " Average Precision  (AP) Channel Marker     @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.350\n",
      " Average Precision  (AP) Channel Marker     @[ IoU=0.50:0.95 | area= small | maxDets=300 ] = 0.293\n",
      " Average Precision  (AP) Channel Marker     @[ IoU=0.50:0.95 | area=medium | maxDets=300 ] = 0.526\n",
      " Average Precision  (AP) Channel Marker     @[ IoU=0.50:0.95 | area= large | maxDets=300 ] = 0.617\n",
      " Average Precision  (AP) Channel Marker     @[ IoU=0.50      | area=   all | maxDets=300 ] = 0.791\n",
      " Average Precision  (AP) Channel Marker     @[ IoU=0.50      | area= small | maxDets=300 ] = 0.742\n",
      " Average Precision  (AP) Channel Marker     @[ IoU=0.50      | area=medium | maxDets=300 ] = 0.969\n",
      " Average Precision  (AP) Channel Marker     @[ IoU=0.50      | area= large | maxDets=300 ] = 0.832\n",
      " Average Precision  (AP) Speed Warning Sign @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.089\n",
      " Average Precision  (AP) Speed Warning Sign @[ IoU=0.50:0.95 | area= small | maxDets=300 ] = 0.057\n",
      " Average Precision  (AP) Speed Warning Sign @[ IoU=0.50:0.95 | area=medium | maxDets=300 ] = 0.190\n",
      " Average Precision  (AP) Speed Warning Sign @[ IoU=0.50:0.95 | area= large | maxDets=300 ] = 0.229\n",
      " Average Precision  (AP) Speed Warning Sign @[ IoU=0.50      | area=   all | maxDets=300 ] = 0.196\n",
      " Average Precision  (AP) Speed Warning Sign @[ IoU=0.50      | area= small | maxDets=300 ] = 0.143\n",
      " Average Precision  (AP) Speed Warning Sign @[ IoU=0.50      | area=medium | maxDets=300 ] = 0.347\n",
      " Average Precision  (AP) Speed Warning Sign @[ IoU=0.50      | area= large | maxDets=300 ] = 0.307\n",
      "\n",
      "+---------------------------------+--------+---------------------------------+--------+---------------------------------+--------+\n",
      "| category                        | AP     | category                        | AP     | category                        | AP     |\n",
      "+---------------------------------+--------+---------------------------------+--------+---------------------------------+--------+\n",
      "| bbox_Buoy_mAP                   | 0.015  | bbox_Buoy_mAP_s                 | 0.017  | bbox_Buoy_mAP_m                 | -1.000 |\n",
      "| bbox_Buoy_mAP_l                 | -1.000 | bbox_Buoy_mAP50                 | 0.054  | bbox_Buoy_mAP50_s               | 0.058  |\n",
      "| bbox_Buoy_mAP50_m               | -1.000 | bbox_Buoy_mAP50_l               | -1.000 | bbox_Boat_mAP                   | 0.511  |\n",
      "| bbox_Boat_mAP_s                 | 0.302  | bbox_Boat_mAP_m                 | 0.495  | bbox_Boat_mAP_l                 | 0.640  |\n",
      "| bbox_Boat_mAP50                 | 0.845  | bbox_Boat_mAP50_s               | 0.630  | bbox_Boat_mAP50_m               | 0.847  |\n",
      "| bbox_Boat_mAP50_l               | 0.924  | bbox_Channel Marker_mAP         | 0.350  | bbox_Channel Marker_mAP_s       | 0.293  |\n",
      "| bbox_Channel Marker_mAP_m       | 0.526  | bbox_Channel Marker_mAP_l       | 0.617  | bbox_Channel Marker_mAP50       | 0.791  |\n",
      "| bbox_Channel Marker_mAP50_s     | 0.742  | bbox_Channel Marker_mAP50_m     | 0.969  | bbox_Channel Marker_mAP50_l     | 0.832  |\n",
      "| bbox_Speed Warning Sign_mAP     | 0.089  | bbox_Speed Warning Sign_mAP_s   | 0.057  | bbox_Speed Warning Sign_mAP_m   | 0.190  |\n",
      "| bbox_Speed Warning Sign_mAP_l   | 0.229  | bbox_Speed Warning Sign_mAP50   | 0.196  | bbox_Speed Warning Sign_mAP50_s | 0.143  |\n",
      "| bbox_Speed Warning Sign_mAP50_m | 0.347  | bbox_Speed Warning Sign_mAP50_l | 0.307  | None                            | None   |\n",
      "+---------------------------------+--------+---------------------------------+--------+---------------------------------+--------+\n",
      "COCO evaluation results are successfully exported to ../../AiBoat/aiboat/APP/NAS/deepstream-services-library/examples/python/eval.json\n"
     ]
    }
   ],
   "source": [
    "evaluate_dict = evaluate(\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    result_json_path=dsl_result_json,\n",
    "    classwise=True,\n",
    "    max_detections=MAX_DETECTIONS,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238c4a1b",
   "metadata": {},
   "source": [
    "## 2. SLICE INFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22343d83",
   "metadata": {},
   "source": [
    "### Slice 좌표 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e5c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image_dir = \"../resources/FLL_VAL/images/\"\n",
    "image_files = sorted([fn for fn in os.listdir(source_image_dir) if fn.endswith(\"jpg\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1510d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from ipywidgets import interact\n",
    "%matplotlib inline \n",
    "from sahi import slicing\n",
    "from sahi.slicing import slice_image\n",
    "\n",
    "slicing.logger.setLevel(slicing.logging.INFO)\n",
    "\n",
    "# single_row_y_start: int = 200,\n",
    "@interact(index=(0, len(image_files)-1), slice_size=(0, 720), overlap_ratio=(0, 0.5, 0.05), single_row_y_start=(0, 512))\n",
    "def visualize_slice_rect(index=0, slice_size=512, overlap_ratio=0.2, single_row_y_start=200):\n",
    "    image_file = image_files[index]\n",
    "    image_path = os.path.join(source_image_dir, image_file)\n",
    "    \n",
    "    res = slice_image(image_path, \n",
    "                      slice_width=slice_size,\n",
    "                      slice_height=slice_size,\n",
    "                      overlap_height_ratio=overlap_ratio,\n",
    "                      overlap_width_ratio=overlap_ratio,\n",
    "                      single_row_y_start=single_row_y_start,\n",
    "                      single_row_predict=True,\n",
    "                      verbose=1)\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    image = copy.deepcopy(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    for start_pixel in res.starting_pixels:\n",
    "        print(start_pixel)\n",
    "        cv2.rectangle(image,\n",
    "                      start_pixel,\n",
    "                      [s1+s2 for s1, s2 in zip(start_pixel,[slice_size,slice_size])],\n",
    "                      color=(255, 0, 0),\n",
    "                      thickness=2)\n",
    "\n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a280d50",
   "metadata": {},
   "source": [
    "### SAHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57ee9120",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"yolov5\"\n",
    "MODEL_PATH = model_path\n",
    "MODEL_CONFIG_PATH = \"\"\n",
    "EVAL_IMAGES_FOLDER_DIR = source_image_dir\n",
    "EVAL_DATASET_JSON_PATH = gt_json_path\n",
    "INFERENCE_SETTING = \"AVIKUS_FL\"\n",
    "EXPORT_VISUAL = False\n",
    "MAX_DETECTIONS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcea216",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_SETTING_TO_PARAMS = {\n",
    "    \"AVIKUS_FL\": {\n",
    "        \"no_standard_prediction\": False,\n",
    "        \"no_sliced_prediction\": False,\n",
    "        \"slice_size\": 640,\n",
    "        \"overlap_ratio\": 0.25,\n",
    "        \"match_threshold\": 0.5,\n",
    "        \"postprocess_class_agnostic\": False,\n",
    "        \"single_row_y_start\": 200,\n",
    "    },\n",
    "}\n",
    "\n",
    "setting_params = INFERENCE_SETTING_TO_PARAMS[INFERENCE_SETTING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b071c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_path=MODEL_PATH,\n",
    "    model_config_path=MODEL_CONFIG_PATH,\n",
    "    model_confidence_threshold=0.4,\n",
    "    model_device=\"cuda:0\",\n",
    "    model_category_mapping=None,\n",
    "    model_category_remapping=None,\n",
    "    source=EVAL_IMAGES_FOLDER_DIR,\n",
    "    no_standard_prediction=setting_params[\"no_standard_prediction\"],\n",
    "    no_sliced_prediction=setting_params[\"no_sliced_prediction\"],\n",
    "#     image_size=,\n",
    "    slice_height=setting_params[\"slice_size\"],\n",
    "    slice_width=setting_params[\"slice_size\"],\n",
    "    overlap_height_ratio=setting_params[\"overlap_ratio\"],\n",
    "    overlap_width_ratio=setting_params[\"overlap_ratio\"],\n",
    "    postprocess_type=\"GREEDYNMM\",\n",
    "    postprocess_match_metric=\"IOS\",\n",
    "#     postprocess_type=\"NMS\",\n",
    "#     postprocess_match_metric=\"IOU\",\n",
    "    postprocess_match_threshold=setting_params[\"match_threshold\"],\n",
    "    postprocess_class_agnostic=setting_params[\"postprocess_class_agnostic\"],\n",
    "    novisual=not EXPORT_VISUAL,\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    project=\"runs/mAP_TEST\",\n",
    "    name=INFERENCE_SETTING,\n",
    "    visual_bbox_thickness=None,\n",
    "    visual_text_size=None,\n",
    "    visual_text_thickness=None,\n",
    "    visual_export_format=\"png\",\n",
    "    verbose=2,\n",
    "    return_dict=True,\n",
    "    force_postprocess_type=True,\n",
    "    single_row_predict=True,\n",
    "    single_row_y_start=setting_params[\"single_row_y_start\"]\n",
    ")\n",
    "\n",
    "result_json_path = str(Path(result[\"export_dir\"]) / \"result.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9878dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dict = evaluate(\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    result_json_path=result_json_path,\n",
    "    classwise=True,\n",
    "    max_detections=MAX_DETECTIONS,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f451b",
   "metadata": {},
   "source": [
    "### DSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf78a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network-input-shape= 5;3;640;640\n",
    "# roi-params-src-0=0;0;1920;1080;0;200;640;640;480;200;640;640;960;200;640;640;1280;200;640;640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b827447",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsl_result_json = \"../../AiBoat/aiboat/APP/NAS/deepstream-services-library/examples/python/result_slice_pred.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c2ec040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.13s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=3.00s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.20s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.191\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=300 ] = 0.387\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=300 ] = 0.160\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=300 ] = 0.311\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=300 ] = 0.583\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=300 ] = 0.577\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=300 ] = 0.126\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=300 ] = 0.319\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=300 ] = 0.391\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=300 ] = 0.175\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=300 ] = 0.364\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=300 ] = 0.449\n",
      " Average Precision  (AP) Buoy               @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.000\n",
      " Average Precision  (AP) Buoy               @[ IoU=0.50:0.95 | area= small | maxDets=300 ] = 0.000\n",
      " Average Precision  (AP) Buoy               @[ IoU=0.50:0.95 | area=medium | maxDets=300 ] = -1.000\n",
      " Average Precision  (AP) Buoy               @[ IoU=0.50:0.95 | area= large | maxDets=300 ] = -1.000\n",
      " Average Precision  (AP) Buoy               @[ IoU=0.50      | area=   all | maxDets=300 ] = 0.000\n",
      " Average Precision  (AP) Buoy               @[ IoU=0.50      | area= small | maxDets=300 ] = 0.000\n",
      " Average Precision  (AP) Buoy               @[ IoU=0.50      | area=medium | maxDets=300 ] = -1.000\n",
      " Average Precision  (AP) Buoy               @[ IoU=0.50      | area= large | maxDets=300 ] = -1.000\n",
      " Average Precision  (AP) Boat               @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.449\n",
      " Average Precision  (AP) Boat               @[ IoU=0.50:0.95 | area= small | maxDets=300 ] = 0.250\n",
      " Average Precision  (AP) Boat               @[ IoU=0.50:0.95 | area=medium | maxDets=300 ] = 0.455\n",
      " Average Precision  (AP) Boat               @[ IoU=0.50:0.95 | area= large | maxDets=300 ] = 0.529\n",
      " Average Precision  (AP) Boat               @[ IoU=0.50      | area=   all | maxDets=300 ] = 0.793\n",
      " Average Precision  (AP) Boat               @[ IoU=0.50      | area= small | maxDets=300 ] = 0.547\n",
      " Average Precision  (AP) Boat               @[ IoU=0.50      | area=medium | maxDets=300 ] = 0.811\n",
      " Average Precision  (AP) Boat               @[ IoU=0.50      | area= large | maxDets=300 ] = 0.855\n",
      " Average Precision  (AP) Channel Marker     @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.315\n",
      " Average Precision  (AP) Channel Marker     @[ IoU=0.50:0.95 | area= small | maxDets=300 ] = 0.256\n",
      " Average Precision  (AP) Channel Marker     @[ IoU=0.50:0.95 | area=medium | maxDets=300 ] = 0.503\n",
      " Average Precision  (AP) Channel Marker     @[ IoU=0.50:0.95 | area= large | maxDets=300 ] = 0.644\n",
      " Average Precision  (AP) Channel Marker     @[ IoU=0.50      | area=   all | maxDets=300 ] = 0.753\n",
      " Average Precision  (AP) Channel Marker     @[ IoU=0.50      | area= small | maxDets=300 ] = 0.698\n",
      " Average Precision  (AP) Channel Marker     @[ IoU=0.50      | area=medium | maxDets=300 ] = 0.937\n",
      " Average Precision  (AP) Channel Marker     @[ IoU=0.50      | area= large | maxDets=300 ] = 0.877\n",
      " Average Precision  (AP) Speed Warning Sign @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.000\n",
      " Average Precision  (AP) Speed Warning Sign @[ IoU=0.50:0.95 | area= small | maxDets=300 ] = 0.000\n",
      " Average Precision  (AP) Speed Warning Sign @[ IoU=0.50:0.95 | area=medium | maxDets=300 ] = 0.000\n",
      " Average Precision  (AP) Speed Warning Sign @[ IoU=0.50:0.95 | area= large | maxDets=300 ] = 0.000\n",
      " Average Precision  (AP) Speed Warning Sign @[ IoU=0.50      | area=   all | maxDets=300 ] = 0.000\n",
      " Average Precision  (AP) Speed Warning Sign @[ IoU=0.50      | area= small | maxDets=300 ] = 0.000\n",
      " Average Precision  (AP) Speed Warning Sign @[ IoU=0.50      | area=medium | maxDets=300 ] = 0.000\n",
      " Average Precision  (AP) Speed Warning Sign @[ IoU=0.50      | area= large | maxDets=300 ] = 0.000\n",
      "\n",
      "+---------------------------------+--------+---------------------------------+--------+---------------------------------+--------+\n",
      "| category                        | AP     | category                        | AP     | category                        | AP     |\n",
      "+---------------------------------+--------+---------------------------------+--------+---------------------------------+--------+\n",
      "| bbox_Buoy_mAP                   | 0.000  | bbox_Buoy_mAP_s                 | 0.000  | bbox_Buoy_mAP_m                 | -1.000 |\n",
      "| bbox_Buoy_mAP_l                 | -1.000 | bbox_Buoy_mAP50                 | 0.000  | bbox_Buoy_mAP50_s               | 0.000  |\n",
      "| bbox_Buoy_mAP50_m               | -1.000 | bbox_Buoy_mAP50_l               | -1.000 | bbox_Boat_mAP                   | 0.449  |\n",
      "| bbox_Boat_mAP_s                 | 0.250  | bbox_Boat_mAP_m                 | 0.455  | bbox_Boat_mAP_l                 | 0.529  |\n",
      "| bbox_Boat_mAP50                 | 0.793  | bbox_Boat_mAP50_s               | 0.547  | bbox_Boat_mAP50_m               | 0.811  |\n",
      "| bbox_Boat_mAP50_l               | 0.855  | bbox_Channel Marker_mAP         | 0.315  | bbox_Channel Marker_mAP_s       | 0.256  |\n",
      "| bbox_Channel Marker_mAP_m       | 0.503  | bbox_Channel Marker_mAP_l       | 0.644  | bbox_Channel Marker_mAP50       | 0.753  |\n",
      "| bbox_Channel Marker_mAP50_s     | 0.698  | bbox_Channel Marker_mAP50_m     | 0.937  | bbox_Channel Marker_mAP50_l     | 0.877  |\n",
      "| bbox_Speed Warning Sign_mAP     | 0.000  | bbox_Speed Warning Sign_mAP_s   | 0.000  | bbox_Speed Warning Sign_mAP_m   | 0.000  |\n",
      "| bbox_Speed Warning Sign_mAP_l   | 0.000  | bbox_Speed Warning Sign_mAP50   | 0.000  | bbox_Speed Warning Sign_mAP50_s | 0.000  |\n",
      "| bbox_Speed Warning Sign_mAP50_m | 0.000  | bbox_Speed Warning Sign_mAP50_l | 0.000  | None                            | None   |\n",
      "+---------------------------------+--------+---------------------------------+--------+---------------------------------+--------+\n",
      "COCO evaluation results are successfully exported to ../../AiBoat/aiboat/APP/NAS/deepstream-services-library/examples/python/eval.json\n"
     ]
    }
   ],
   "source": [
    "evaluate_dict = evaluate(\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    result_json_path=dsl_result_json,\n",
    "    classwise=True,\n",
    "    max_detections=MAX_DETECTIONS,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b155c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f414db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a41f44c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
