{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f461dc42",
   "metadata": {},
   "source": [
    "# SAHI Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b513e",
   "metadata": {},
   "source": [
    "## 1. FULL INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71949b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8efb89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrange an instance segmentation model for test\n",
    "from sahi.utils.yolov5 import (\n",
    "    download_yolov5s6_model,\n",
    ")\n",
    "\n",
    "# import required functions, classes\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.utils.cv import read_image\n",
    "from sahi.utils.file import download_from_url\n",
    "from sahi.predict import get_prediction, get_sliced_prediction, predict\n",
    "from sahi.scripts.coco_error_analysis import analyse\n",
    "from sahi.scripts.coco_evaluation import evaluate\n",
    "from IPython.display import Image\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f251a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image_dir = \"../resources/FLL_VAL/images/\"\n",
    "source_label_dir = \"../resources/FLL_VAL/labels/\"\n",
    "\n",
    "coco_m_path = '../resources/models/yolov5m.pt'\n",
    "fll_model_221007_path = '../resources/models/221007/best.pt'\n",
    "fll_model_221012_path = '../resources/models/221012/best.pt'\n",
    "fll_model_221013_path = '../resources/models/221013/best.pt'\n",
    "fll_model_221014_path = '../resources/models/221014/best.pt'\n",
    "fll_model_221021_path = '../resources/models/221021/best.pt'\n",
    "fll_model_221021_960_path = '../resources/models/221021_960/best.pt'\n",
    "fll_model_221022_960_path = '../resources/models/221022_960/best.pt'\n",
    "fll_model_221024_960_path = '../resources/models/221024_960/best.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa69f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "fll_model_221024_960 = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov5',\n",
    "    model_path=fll_model_221024_960_path,\n",
    "    confidence_threshold=0.25,\n",
    "    device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa71aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fll_model_221024_960\n",
    "model_path = fll_model_221024_960_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_extract(img_dir, label_dir, out_dir):\n",
    "    if os.path.exists(os.path.join(out_dir, 'val.json')):\n",
    "        os.remove(os.path.join(out_dir, 'val.json'))\n",
    "    \n",
    "    licenses = [\n",
    "        {\n",
    "            \"name\": \"\",\n",
    "            \"id\": 0,\n",
    "            \"url\": \"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    info_ = [\n",
    "        {\n",
    "            \"contributor\": \"\",\n",
    "            \"date_created\": \"\",\n",
    "            \"description\": \"\",\n",
    "            \"url\": \"\",\n",
    "            \"version\": \"\",\n",
    "            \"year\": \"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    categories = [\n",
    "        {\n",
    "            \"id\": 0,\n",
    "            \"name\": \"Buoy\",\n",
    "            \"supercategory\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"name\": \"Boat\",\n",
    "            \"supercategory\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"name\": \"Channel Marker\",\n",
    "            \"supercategory\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 3,\n",
    "            \"name\": \"Speed Warning Sign\",\n",
    "            \"supercategory\": \"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    img_idx = 0\n",
    "    annot_idx = 0\n",
    "\n",
    "    imgs_list = []\n",
    "    annots_list = []\n",
    "\n",
    "    for label_file in sorted(os.listdir(label_dir)):\n",
    "        label_file_ = os.path.join(label_dir, label_file)\n",
    "        img_file_ = os.path.join(img_dir, f'{os.path.splitext(label_file)[0]}.jpg')\n",
    "        img = Image.open(img_file_)\n",
    "        image_w, image_h = img.size\n",
    "\n",
    "        imgs_list.append({\n",
    "            'id': img_idx,\n",
    "            'width': image_w,\n",
    "            'height': image_h,\n",
    "            'file_name': f'{os.path.splitext(label_file)[0]}.jpg',\n",
    "            \"license\": 0,\n",
    "            \"flickr_url\": \"\",\n",
    "            \"coco_url\": \"\",\n",
    "            \"date_captured\": 0\n",
    "        })\n",
    "\n",
    "        with open(label_file_, 'r') as label_f:\n",
    "            labels = label_f.readlines()\n",
    "\n",
    "            for label in labels:\n",
    "                cat, xc, yc, label_normalized_w, label_normalized_h = list(map(lambda x: int(x) if len(x) == 1 else float(x), label.split()))\n",
    "                label_w, label_h = image_w * label_normalized_w, image_h * label_normalized_h\n",
    "                xmin, ymin = (image_w * xc) - (label_w / 2), (image_h * yc) - (label_h / 2)\n",
    "                \n",
    "                xmin = 0 if xmin < 0 else xmin\n",
    "                ymin = 0 if ymin < 0 else ymin\n",
    "\n",
    "                annots_list.append({\n",
    "                    'id': annot_idx,\n",
    "                    'image_id': img_idx,\n",
    "                    'category_id': cat,\n",
    "                    'area': int(label_h * label_w),\n",
    "                    'bbox': [\n",
    "                        xmin,\n",
    "                        ymin,\n",
    "                        label_w,\n",
    "                        label_h\n",
    "                    ],\n",
    "                    'iscrowd': 0,\n",
    "                    'attributes': {\n",
    "                        'type': '',\n",
    "                        'occluded': False\n",
    "                    },\n",
    "                    'segmentation': []\n",
    "                })\n",
    "\n",
    "                annot_idx += 1\n",
    "\n",
    "        img_idx += 1\n",
    "\n",
    "    out_dict = {\n",
    "        'licenses': licenses,\n",
    "        'info': info_,\n",
    "        'categories': categories,\n",
    "        'images': imgs_list,\n",
    "        'annotations': annots_list\n",
    "    }\n",
    "    \n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(out_dir, 'val.json'), 'w') as out_f:\n",
    "        print(os.path.join(out_dir, 'val.json'))\n",
    "        json.dump(out_dict, out_f)\n",
    "        \n",
    "    return os.path.join(out_dir, 'val.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de067ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_extract(img_dir, label_dir, out_dir)\n",
    "gt_json_path = initial_extract(source_image_dir, source_label_dir, str(Path(source_image_dir).parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd4e997",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"yolov5\"\n",
    "MODEL_PATH = model_path\n",
    "MODEL_CONFIG_PATH = \"\"\n",
    "EVAL_IMAGES_FOLDER_DIR = source_image_dir\n",
    "EVAL_DATASET_JSON_PATH = gt_json_path\n",
    "INFERENCE_SETTING = \"AVIKUS_FL\"\n",
    "EXPORT_VISUAL = False\n",
    "MAX_DETECTIONS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d67035",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_SETTING_TO_PARAMS = {\n",
    "    \"AVIKUS_FL\": {\n",
    "        \"no_standard_prediction\": False,\n",
    "        \"no_sliced_prediction\": True,\n",
    "        \"slice_size\": 512,\n",
    "        \"overlap_ratio\": 0.15,\n",
    "        \"match_threshold\": 0.5,\n",
    "        \"postprocess_class_agnostic\": False,\n",
    "        \"custom_slice_y_start\": 200,\n",
    "    },\n",
    "}\n",
    "\n",
    "setting_params = INFERENCE_SETTING_TO_PARAMS[INFERENCE_SETTING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085fba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_path=MODEL_PATH,\n",
    "    model_config_path=MODEL_CONFIG_PATH,\n",
    "    model_confidence_threshold=0.25,\n",
    "    model_device=\"cuda:0\",\n",
    "    model_category_mapping=None,\n",
    "    model_category_remapping=None,\n",
    "    source=EVAL_IMAGES_FOLDER_DIR,\n",
    "    no_standard_prediction=setting_params[\"no_standard_prediction\"],\n",
    "    no_sliced_prediction=setting_params[\"no_sliced_prediction\"],\n",
    "    slice_height=setting_params[\"slice_size\"],\n",
    "    slice_width=setting_params[\"slice_size\"],\n",
    "    overlap_height_ratio=setting_params[\"overlap_ratio\"],\n",
    "    overlap_width_ratio=setting_params[\"overlap_ratio\"],\n",
    "    image_size=960,\n",
    "    postprocess_type=\"GREEDYNMM\",\n",
    "    postprocess_match_metric=\"IOS\",\n",
    "    postprocess_match_threshold=setting_params[\"match_threshold\"],\n",
    "    postprocess_class_agnostic=setting_params[\"postprocess_class_agnostic\"],\n",
    "    novisual=not EXPORT_VISUAL,\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    project=\"runs/FLL_221024_960_0.25_STANDARD\",\n",
    "    name=INFERENCE_SETTING,\n",
    "    visual_bbox_thickness=None,\n",
    "    visual_text_size=None,\n",
    "    visual_text_thickness=None,\n",
    "    visual_export_format=\"png\",\n",
    "    verbose=2,\n",
    "    return_dict=True,\n",
    "    force_postprocess_type=True,\n",
    "    custom_slice_mode=0,\n",
    "    custom_slice_x_start=0,\n",
    "    custom_slice_y_start=0,\n",
    ")\n",
    "\n",
    "result_json_path = str(Path(result[\"export_dir\"]) / \"result.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dict = evaluate(\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    result_json_path=result_json_path,\n",
    "    classwise=True,\n",
    "    max_detections=MAX_DETECTIONS,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c844485",
   "metadata": {},
   "source": [
    "## Mode 2 (One Box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab7483",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_SETTING_TO_PARAMS = {\n",
    "    \"AVIKUS_FL\": {\n",
    "        \"no_standard_prediction\": False,\n",
    "        \"no_sliced_prediction\": False,\n",
    "        \"slice_size\": 512,\n",
    "        \"overlap_ratio\": 0.15,\n",
    "        \"match_threshold\": 0.5,\n",
    "        \"postprocess_class_agnostic\": False,\n",
    "        \"custom_slice_x_start\": 640,\n",
    "        \"custom_slice_y_start\": 360,\n",
    "        \"custom_slice_mode\": 2\n",
    "    },\n",
    "}\n",
    "\n",
    "setting_params = INFERENCE_SETTING_TO_PARAMS[INFERENCE_SETTING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93797401",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_path=MODEL_PATH,\n",
    "    model_config_path=MODEL_CONFIG_PATH,\n",
    "    model_confidence_threshold=0.25,\n",
    "    model_device=\"cuda:0\",\n",
    "    model_category_mapping=None,\n",
    "    model_category_remapping=None,\n",
    "    source=EVAL_IMAGES_FOLDER_DIR,\n",
    "    no_standard_prediction=setting_params[\"no_standard_prediction\"],\n",
    "    no_sliced_prediction=setting_params[\"no_sliced_prediction\"],\n",
    "    slice_height=setting_params[\"slice_size\"],\n",
    "    slice_width=setting_params[\"slice_size\"],\n",
    "    overlap_height_ratio=setting_params[\"overlap_ratio\"],\n",
    "    overlap_width_ratio=setting_params[\"overlap_ratio\"],\n",
    "    postprocess_type=\"GREEDYNMM\",\n",
    "    postprocess_match_metric=\"IOS\",\n",
    "    postprocess_match_threshold=setting_params[\"match_threshold\"],\n",
    "    postprocess_class_agnostic=setting_params[\"postprocess_class_agnostic\"],\n",
    "    novisual=not EXPORT_VISUAL,\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    project=\"runs/mode0\",\n",
    "    name=INFERENCE_SETTING,\n",
    "    visual_bbox_thickness=None,\n",
    "    visual_text_size=None,\n",
    "    visual_text_thickness=None,\n",
    "    visual_export_format=\"png\",\n",
    "    verbose=2,\n",
    "    return_dict=True,\n",
    "    force_postprocess_type=True,\n",
    "    custom_slice_mode=setting_params[\"custom_slice_mode\"],\n",
    "    custom_slice_x_start=setting_params[\"custom_slice_x_start\"],\n",
    "    custom_slice_y_start=setting_params[\"custom_slice_y_start\"],\n",
    ")\n",
    "\n",
    "result_json_path = str(Path(result[\"export_dir\"]) / \"result.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036d7643",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dict = evaluate(\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    result_json_path=result_json_path,\n",
    "    classwise=True,\n",
    "    max_detections=MAX_DETECTIONS,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc75328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_path=MODEL_PATH,\n",
    "    model_config_path=MODEL_CONFIG_PATH,\n",
    "    model_confidence_threshold=0.25,\n",
    "    model_device=\"cuda:0\",\n",
    "    model_category_mapping=None,\n",
    "    model_category_remapping=None,\n",
    "    source=EVAL_IMAGES_FOLDER_DIR,\n",
    "    no_standard_prediction=setting_params[\"no_standard_prediction\"],\n",
    "    no_sliced_prediction=setting_params[\"no_sliced_prediction\"],\n",
    "    slice_height=setting_params[\"slice_size\"],\n",
    "    slice_width=setting_params[\"slice_size\"],\n",
    "    overlap_height_ratio=setting_params[\"overlap_ratio\"],\n",
    "    overlap_width_ratio=setting_params[\"overlap_ratio\"],\n",
    "    postprocess_type=\"NMM\",\n",
    "    postprocess_match_metric=\"IOS\",\n",
    "    postprocess_match_threshold=setting_params[\"match_threshold\"],\n",
    "    postprocess_class_agnostic=setting_params[\"postprocess_class_agnostic\"],\n",
    "    novisual=not EXPORT_VISUAL,\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    project=\"runs/mode0\",\n",
    "    name=INFERENCE_SETTING,\n",
    "    visual_bbox_thickness=None,\n",
    "    visual_text_size=None,\n",
    "    visual_text_thickness=None,\n",
    "    visual_export_format=\"png\",\n",
    "    verbose=2,\n",
    "    return_dict=True,\n",
    "    force_postprocess_type=True,\n",
    "    custom_slice_mode=setting_params[\"custom_slice_mode\"],\n",
    "    custom_slice_x_start=setting_params[\"custom_slice_x_start\"],\n",
    "    custom_slice_y_start=setting_params[\"custom_slice_y_start\"],\n",
    ")\n",
    "\n",
    "result_json_path = str(Path(result[\"export_dir\"]) / \"result.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bab3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dict = evaluate(\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    result_json_path=result_json_path,\n",
    "    classwise=True,\n",
    "    max_detections=MAX_DETECTIONS,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94e3f82",
   "metadata": {},
   "source": [
    "## Mode 3 (Four Box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7663ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_SETTING_TO_PARAMS = {\n",
    "    \"AVIKUS_FL\": {\n",
    "        \"no_standard_prediction\": False,\n",
    "        \"no_sliced_prediction\": False,\n",
    "        \"slice_size\": 360,\n",
    "        \"overlap_ratio\": 0.15,\n",
    "        \"match_threshold\": 0.5,\n",
    "        \"postprocess_class_agnostic\": False,\n",
    "        \"custom_slice_x_start\": 580,\n",
    "        \"custom_slice_y_start\": 280,\n",
    "        \"custom_slice_mode\": 3\n",
    "    },\n",
    "}\n",
    "\n",
    "setting_params = INFERENCE_SETTING_TO_PARAMS[INFERENCE_SETTING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd0cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_path=MODEL_PATH,\n",
    "    model_config_path=MODEL_CONFIG_PATH,\n",
    "    model_confidence_threshold=0.001,\n",
    "    model_device=\"cuda:0\",\n",
    "    model_category_mapping=None,\n",
    "    model_category_remapping=None,\n",
    "    source=EVAL_IMAGES_FOLDER_DIR,\n",
    "    no_standard_prediction=setting_params[\"no_standard_prediction\"],\n",
    "    no_sliced_prediction=setting_params[\"no_sliced_prediction\"],\n",
    "    slice_height=setting_params[\"slice_size\"],\n",
    "    slice_width=setting_params[\"slice_size\"],\n",
    "    overlap_height_ratio=setting_params[\"overlap_ratio\"],\n",
    "    overlap_width_ratio=setting_params[\"overlap_ratio\"],\n",
    "    postprocess_type=\"GREEDYNMM\",\n",
    "    postprocess_match_metric=\"IOS\",\n",
    "    postprocess_match_threshold=setting_params[\"match_threshold\"],\n",
    "    postprocess_class_agnostic=setting_params[\"postprocess_class_agnostic\"],\n",
    "    novisual=not EXPORT_VISUAL,\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    project=\"runs/mode3\",\n",
    "    name=INFERENCE_SETTING,\n",
    "    visual_bbox_thickness=None,\n",
    "    visual_text_size=None,\n",
    "    visual_text_thickness=None,\n",
    "    visual_export_format=\"png\",\n",
    "    verbose=2,\n",
    "    return_dict=True,\n",
    "    force_postprocess_type=True,\n",
    "    custom_slice_mode=setting_params[\"custom_slice_mode\"],\n",
    "    custom_slice_x_start=setting_params[\"custom_slice_x_start\"],\n",
    "    custom_slice_y_start=setting_params[\"custom_slice_y_start\"],\n",
    ")\n",
    "\n",
    "result_json_path = str(Path(result[\"export_dir\"]) / \"result.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb2152",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dict = evaluate(\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    result_json_path=result_json_path,\n",
    "    classwise=True,\n",
    "    max_detections=MAX_DETECTIONS,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9eea1e",
   "metadata": {},
   "source": [
    "## DSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22b6579",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsl_result_json = \"../../AiBoat/aiboat/APP/NAS/deepstream-services-library/examples/python/result_FL221024_letterbox_0.25_standard_pred.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947afd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dict = evaluate(\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    result_json_path=dsl_result_json,\n",
    "    classwise=True,\n",
    "    max_detections=MAX_DETECTIONS,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f7d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsl_result_json3 = \"../../AiBoat/aiboat/APP/NAS/deepstream-services-library/examples/python/result_YOLOV3_letterbox_0.25_standard_pred.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed60661",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dict = evaluate(\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    result_json_path=dsl_result_json3,\n",
    "    classwise=True,\n",
    "    max_detections=MAX_DETECTIONS,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31610f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsl_result_json2 = \"../../AiBoat/aiboat/APP/NAS/deepstream-services-library/examples/python/result_YOLOV3_0.25_slice_pred.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77fabc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dict = evaluate(\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    result_json_path=dsl_result_json2,\n",
    "    classwise=True,\n",
    "    max_detections=MAX_DETECTIONS,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238c4a1b",
   "metadata": {},
   "source": [
    "## 2. SLICE INFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22343d83",
   "metadata": {},
   "source": [
    "### Slice 좌표 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e5c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image_dir = \"../resources/FLL_VAL/images/\"\n",
    "image_files = sorted([fn for fn in os.listdir(source_image_dir) if fn.endswith(\"jpg\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1510d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from ipywidgets import interact\n",
    "%matplotlib inline \n",
    "from sahi import slicing\n",
    "from sahi.slicing import slice_image\n",
    "\n",
    "slicing.logger.setLevel(slicing.logging.INFO)\n",
    "\n",
    "# single_row_y_start: int = 200,\n",
    "@interact(index=(0, len(image_files)-1), slice_size=(0, 720), overlap_ratio=(0, 0.5, 0.05), single_row_y_start=(0, 512))\n",
    "def visualize_slice_rect(index=0, slice_size=512, overlap_ratio=0.2, single_row_y_start=200):\n",
    "    image_file = image_files[index]\n",
    "    image_path = os.path.join(source_image_dir, image_file)\n",
    "    \n",
    "    res = slice_image(image_path, \n",
    "                      slice_width=slice_size,\n",
    "                      slice_height=slice_size,\n",
    "                      overlap_height_ratio=overlap_ratio,\n",
    "                      overlap_width_ratio=overlap_ratio,\n",
    "                      single_row_y_start=single_row_y_start,\n",
    "                      single_row_predict=True,\n",
    "                      verbose=1)\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    image = copy.deepcopy(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    for start_pixel in res.starting_pixels:\n",
    "        print(start_pixel)\n",
    "        cv2.rectangle(image,\n",
    "                      start_pixel,\n",
    "                      [s1+s2 for s1, s2 in zip(start_pixel,[slice_size,slice_size])],\n",
    "                      color=(255, 0, 0),\n",
    "                      thickness=2)\n",
    "\n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a280d50",
   "metadata": {},
   "source": [
    "### SAHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ee9120",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"yolov5\"\n",
    "MODEL_PATH = model_path\n",
    "MODEL_CONFIG_PATH = \"\"\n",
    "EVAL_IMAGES_FOLDER_DIR = source_image_dir\n",
    "EVAL_DATASET_JSON_PATH = gt_json_path\n",
    "INFERENCE_SETTING = \"AVIKUS_FL\"\n",
    "EXPORT_VISUAL = False\n",
    "MAX_DETECTIONS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcea216",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_SETTING_TO_PARAMS = {\n",
    "    \"AVIKUS_FL\": {\n",
    "        \"no_standard_prediction\": False,\n",
    "        \"no_sliced_prediction\": False,\n",
    "        \"slice_size\": 640,\n",
    "        \"overlap_ratio\": 0.25,\n",
    "        \"match_threshold\": 0.5,\n",
    "        \"postprocess_class_agnostic\": False,\n",
    "        \"single_row_y_start\": 200,\n",
    "    },\n",
    "}\n",
    "\n",
    "setting_params = INFERENCE_SETTING_TO_PARAMS[INFERENCE_SETTING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b071c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_path=MODEL_PATH,\n",
    "    model_config_path=MODEL_CONFIG_PATH,\n",
    "    model_confidence_threshold=0.4,\n",
    "    model_device=\"cuda:0\",\n",
    "    model_category_mapping=None,\n",
    "    model_category_remapping=None,\n",
    "    source=EVAL_IMAGES_FOLDER_DIR,\n",
    "    no_standard_prediction=setting_params[\"no_standard_prediction\"],\n",
    "    no_sliced_prediction=setting_params[\"no_sliced_prediction\"],\n",
    "#     image_size=,\n",
    "    slice_height=setting_params[\"slice_size\"],\n",
    "    slice_width=setting_params[\"slice_size\"],\n",
    "    overlap_height_ratio=setting_params[\"overlap_ratio\"],\n",
    "    overlap_width_ratio=setting_params[\"overlap_ratio\"],\n",
    "    postprocess_type=\"GREEDYNMM\",\n",
    "    postprocess_match_metric=\"IOS\",\n",
    "#     postprocess_type=\"NMS\",\n",
    "#     postprocess_match_metric=\"IOU\",\n",
    "    postprocess_match_threshold=setting_params[\"match_threshold\"],\n",
    "    postprocess_class_agnostic=setting_params[\"postprocess_class_agnostic\"],\n",
    "    novisual=not EXPORT_VISUAL,\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    project=\"runs/mAP_TEST\",\n",
    "    name=INFERENCE_SETTING,\n",
    "    visual_bbox_thickness=None,\n",
    "    visual_text_size=None,\n",
    "    visual_text_thickness=None,\n",
    "    visual_export_format=\"png\",\n",
    "    verbose=2,\n",
    "    return_dict=True,\n",
    "    force_postprocess_type=True,\n",
    "    single_row_predict=True,\n",
    "    single_row_y_start=setting_params[\"single_row_y_start\"]\n",
    ")\n",
    "\n",
    "result_json_path = str(Path(result[\"export_dir\"]) / \"result.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9878dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dict = evaluate(\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    result_json_path=result_json_path,\n",
    "    classwise=True,\n",
    "    max_detections=MAX_DETECTIONS,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f451b",
   "metadata": {},
   "source": [
    "### DSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf78a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network-input-shape= 5;3;640;640\n",
    "# roi-params-src-0=0;0;1920;1080;0;200;640;640;480;200;640;640;960;200;640;640;1280;200;640;640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b827447",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsl_result_json = \"../../AiBoat/aiboat/APP/NAS/deepstream-services-library/examples/python/result_slice_pred.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ec040",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dict = evaluate(\n",
    "    dataset_json_path=EVAL_DATASET_JSON_PATH,\n",
    "    result_json_path=dsl_result_json,\n",
    "    classwise=True,\n",
    "    max_detections=MAX_DETECTIONS,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b155c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f414db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a41f44c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
