{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eebc0d0",
   "metadata": {},
   "source": [
    "# 1. 비디오 불러와서 프레임 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb07ec09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yyj-avikus/sahi/demo'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd9ba4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def extract_frame(video_path, frame_dir, overwrite=False, start=-1, end=-1, every=1):\n",
    "    \"\"\"\n",
    "    Extract frames from a video using OpenCVs VideoCapture\n",
    "    :param video_path: path of the video\n",
    "    :param frames_dir: the directory to save the frames\n",
    "    :param overwrite: to overwrite frames that already exist?\n",
    "    :param start: start frame\n",
    "    :param end: end frame\n",
    "    :param every: frame spacing\n",
    "    :return: count of images saved\n",
    "    \"\"\"\n",
    "    \n",
    "    video_path = os.path.normpath(video_path)\n",
    "    frame_dir = os.path.normpath(frame_dir)\n",
    "    \n",
    "    video_dir, video_filename = os.path.split(video_path)\n",
    "    video_filename = os.path.splitext(video_filename)[0]\n",
    "    assert os.path.exists(video_path)\n",
    "    \n",
    "    capture = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if start < 0:\n",
    "        start = 0\n",
    "    if end < 0:\n",
    "        end = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    print(fps)\n",
    "    \n",
    "    capture.set(cv2.CAP_PROP_FPS,30) \n",
    "    print(capture.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    print(start, end)\n",
    "    capture.set(1, start)\n",
    "    frame = start\n",
    "    while_safety = 0\n",
    "    saved_count = 0 \n",
    "    \n",
    "    while frame < end:\n",
    "        if while_safety > 10:  # break the while if our safety maxs out at 10\n",
    "            break\n",
    "        \n",
    "        _, image = capture.read()\n",
    "        \n",
    "        if frame % every == 0:\n",
    "           \n",
    "            if image is None:\n",
    "                print(\"Image is None\")\n",
    "                while_safety += 1\n",
    "                continue\n",
    "            \n",
    "            while_safety = 0\n",
    "            \n",
    "            save_path = os.path.join(frame_dir, video_filename, \"{:010d}.jpg\".format(saved_count))\n",
    "            \n",
    "            if not os.path.exists(os.path.join(frame_dir, video_filename)):\n",
    "                os.makedirs(os.path.join(frame_dir, video_filename))\n",
    "            if not os.path.exists(save_path) or overwrite:\n",
    "                cv2.imwrite(save_path, image)\n",
    "                saved_count += 1\n",
    "                \n",
    "        frame += 1\n",
    "        \n",
    "    capture.release()\n",
    "        \n",
    "    return saved_count, os.path.join(frame_dir, video_filename);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7439f748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.00066428692063\n",
      "30.00066428692063\n",
      "0 38088\n"
     ]
    }
   ],
   "source": [
    "video_path = \"../resources/FL_221017_2.mp4\"\n",
    "frame_dirs = \"../resources/export_frame\"\n",
    "\n",
    "_, source_image_dir = extract_frame(video_path, frame_dirs, start=0, every=3, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a9d908e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../resources/export_frame/FL_221017_2/'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_image_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd748d9",
   "metadata": {},
   "source": [
    "# 2. 시작, 끝 프레임 지정해서 영상 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e5a5bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 687/687 [00:20<00:00, 34.13it/s]\n"
     ]
    }
   ],
   "source": [
    "fps=30\n",
    "out = cv2.VideoWriter('FL_221017_2_track01.mp4',cv2.VideoWriter_fourcc(*'mp4v'), fps, (1920,1080))\n",
    "# //3이 된 값\n",
    "start, end = 2978, 3664\n",
    "\n",
    "for filename in tqdm(sorted(glob.glob(source_image_dir+\"*.jpg\"))[start:end+1]):\n",
    "    img = cv2.imread(filename)    \n",
    "    out.write(img)\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f1e191",
   "metadata": {},
   "source": [
    "# 3. 비디오 인퍼런스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "649c59db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrange an instance segmentation model for test\n",
    "from sahi.utils.yolov5 import (\n",
    "    download_yolov5s6_model,\n",
    ")\n",
    "\n",
    "# import required functions, classes\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.utils.cv import read_image\n",
    "from sahi.utils.file import download_from_url\n",
    "from sahi.predict import get_prediction, get_sliced_prediction, predict\n",
    "from sahi.scripts.coco_error_analysis import analyse\n",
    "from sahi.scripts.coco_evaluation import evaluate\n",
    "from sahi.slicing import slice_image\n",
    "from IPython.display import Image\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d075d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "fll_model_221107_track_path = '../resources/models/221107_track/best.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b300a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fll_model_221107_track = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov5',\n",
    "    model_path=fll_model_221107_track_path,\n",
    "    confidence_threshold=0.25,\n",
    "    device=\"cuda:0\",\n",
    "    image_size=960\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc0e53cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fll_model_221107_track\n",
    "model_path = fll_model_221107_track_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be11f620",
   "metadata": {},
   "source": [
    "## Interact로 인퍼런스 샘플 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3e88a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sahi.utils.cv import Colors\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def visualize_object_predictions(\n",
    "    image: np.array,\n",
    "    object_prediction_list,\n",
    "    rect_th: int = None,\n",
    "    text_size: float = None,\n",
    "    text_th: float = None,\n",
    "    color: tuple = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualizes prediction category names, bounding boxes over the source image\n",
    "    and exports it to output folder.\n",
    "    Arguments:\n",
    "        object_prediction_list: a list of prediction.ObjectPrediction\n",
    "        rect_th: rectangle thickness\n",
    "        text_size: size of the category name over box\n",
    "        text_th: text thickness\n",
    "        color: annotation color in the form: (0, 255, 0)\n",
    "        output_dir: directory for resulting visualization to be exported\n",
    "        file_name: exported file will be saved as: output_dir+file_name+\".png\"\n",
    "        export_format: can be specified as 'jpg' or 'png'\n",
    "    \"\"\"\n",
    "    # deepcopy image so that original is not altered\n",
    "    image = copy.deepcopy(image)\n",
    "    # select predefined classwise color palette if not specified\n",
    "    if color is None:\n",
    "        colors = Colors()\n",
    "    else:\n",
    "        colors = None\n",
    "    # set rect_th for boxes\n",
    "    rect_th = rect_th or max(round(sum(image.shape) / 2 * 0.001), 1)\n",
    "    # set text_th for category names\n",
    "    text_th = text_th or max(rect_th - 1, 1)\n",
    "    # set text_size for category names\n",
    "    text_size = text_size or rect_th / 3\n",
    "    # add bbox and mask to image if present\n",
    "    for object_prediction in object_prediction_list:\n",
    "        # deepcopy object_prediction_list so that original is not altered\n",
    "        object_prediction = object_prediction.deepcopy()\n",
    "\n",
    "        bbox = object_prediction.bbox.to_voc_bbox()\n",
    "        category_name = object_prediction.category.name\n",
    "        score = object_prediction.score.value\n",
    "\n",
    "        # set color\n",
    "        if colors is not None:\n",
    "            color = colors(object_prediction.category.id)\n",
    "        # visualize masks if present\n",
    "        if object_prediction.mask is not None:\n",
    "            # deepcopy mask so that original is not altered\n",
    "            mask = object_prediction.mask.bool_mask\n",
    "            # draw mask\n",
    "            rgb_mask = apply_color_mask(mask, color)\n",
    "            image = cv2.addWeighted(image, 1, rgb_mask, 0.4, 0)\n",
    "        # set bbox points\n",
    "        p1, p2 = (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3]))\n",
    "        # visualize boxes\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            p1,\n",
    "            p2,\n",
    "            color=color,\n",
    "            thickness=rect_th\n",
    "        )\n",
    "        # arange bounding box text location\n",
    "        label = f\"{category_name} {score:.2f}\"\n",
    "        w, h = cv2.getTextSize(label, 0, fontScale=text_size, thickness=text_th)[0]  # label width, height\n",
    "        outside = p1[1] - h - 3 >= 0  # label fits outside box\n",
    "        p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\n",
    "        # add bounding box text\n",
    "        cv2.rectangle(image, p1, p2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            label,\n",
    "            (p1[0], p1[1] - 2 if outside else p1[1] + h + 2),\n",
    "            0,\n",
    "            text_size,\n",
    "            (255, 255, 255),\n",
    "            thickness=text_th,\n",
    "        )\n",
    "        \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86d25590",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_val_image_dir = '../resources/save_fl221017_2/images/'\n",
    "save_val_label_dir = '../resources/save_fl221017_2/labels/'\n",
    "source_image_dir = \"../resources/export_frame/FL_221017_2/\"\n",
    "\n",
    "os.makedirs(save_val_image_dir, exist_ok=True)\n",
    "os.makedirs(save_val_label_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4beb239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f024f56bd9e24b23b71df3a44177dd42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2978, description='index', max=3664, min=2978), IntSlider(value=0, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# some handy functions to use along widgets\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "# widget packages\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "size=(1920,1080)\n",
    "image_files = sorted(os.listdir(source_image_dir))\n",
    "# label_files = sorted([fn for fn in os.listdir(source_label_dir) if fn.endswith(\"txt\")])\n",
    "\n",
    "def convert(size, box):\n",
    "    dw = 1./(size[0])\n",
    "    dh = 1./(size[1])\n",
    "    x = (box[0] + box[1])/2.0 - 1\n",
    "    y = (box[2] + box[3])/2.0 - 1\n",
    "    w = box[1] - box[0]\n",
    "    h = box[3] - box[2]\n",
    "    x = x*dw\n",
    "    w = w*dw\n",
    "    y = y*dh\n",
    "    h = h*dh\n",
    "    return (x,y,w,h)\n",
    "\n",
    "@interact(index=(start, end), save=(0,1))\n",
    "def show_sample(index=0, save=0):\n",
    "    image_file = image_files[index]\n",
    "    image_path = os.path.join(source_image_dir, image_file)\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    result = get_prediction(image_path, model)\n",
    "    \n",
    "    if save == 1:\n",
    "        label_file_name = \"FL221017_2_\"+os.path.splitext(image_file)[0]+\".txt\"\n",
    "        image_file_name = f\"FL221017_2_{image_files[index]}\"\n",
    "\n",
    "        shutil.copy(image_path, save_val_image_dir+image_file_name)\n",
    "    \n",
    "        Path(save_val_label_dir+label_file_name).touch()\n",
    "        with open(save_val_label_dir+label_file_name, 'w') as f:    \n",
    "            for object_prediction in result.object_prediction_list:\n",
    "                object_prediction = object_prediction.deepcopy()\n",
    "                bbox = object_prediction.bbox.to_voc_bbox()\n",
    "                category_id = object_prediction.category.id\n",
    "                yolo = convert(size,[bbox[0], bbox[2], bbox[1], bbox[3]])\n",
    "                f.write(f\"{category_id} {yolo[0]} {yolo[1]} {yolo[2]} {yolo[3]}\\n\")\n",
    "                \n",
    "        print(save_val_image_dir+image_file_name, save_val_label_dir+label_file_name)\n",
    "\n",
    "    canvas = visualize_object_predictions(image, result.object_prediction_list)\n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.imshow(canvas)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3ba8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "coco=COCO(gt_json_path)\n",
    "y_offset=-20\n",
    "\n",
    "@interact(index=(0, len(image_files)-1))\n",
    "def draw_gt_bbox(index=0):\n",
    "    fig, ax = plt.subplots()\n",
    "    img = coco.loadImgs(ids=[index])[-1]\n",
    "    I = cv2.imread(os.path.join(source_image_dir, img['file_name']))\n",
    "    I = cv2.cvtColor(I, cv2.COLOR_BGR2RGB)\n",
    "    ax.imshow(I); plt.axis('off')\n",
    "    annIds = coco.getAnnIds(imgIds=img['id'], iscrowd=None)\n",
    "    anns = coco.loadAnns(annIds)\n",
    "    coco.showAnns(anns, draw_bbox=True)\n",
    "    for i, ann in enumerate(anns):\n",
    "        ax.text(anns[i]['bbox'][0], anns[i]['bbox'][1]+y_offset, anns[i]['category_id'], style='italic', \n",
    "                bbox={'facecolor': 'white', 'alpha': 0.7, 'pad': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b79211",
   "metadata": {},
   "source": [
    "## remove no label images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218b79f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for image_file in os.listdir(save_val_image_dir):\n",
    "    label_file = save_val_label_dir+os.path.splitext(image_file)[0]+\".txt\"\n",
    "    cnt += os.path.exists(label_file)\n",
    "    if not os.path.exists(label_file):\n",
    "        print(\"label file does not exit\")\n",
    "        os.remove(os.path.join(save_val_image_dir,image_file))\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635b05ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir(save_val_image_dir)) == len(os.listdir(save_val_label_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502d258f",
   "metadata": {},
   "source": [
    "## 기존 VAL에서 복사한 이미지들 삭제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f3e618",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_name in sorted(os.listdir(save_val_image_dir)):\n",
    "    label_file = save_val_label_dir+os.path.splitext(image_name)[0][4:]+\".txt\"\n",
    "    image_file = os.path.join(save_val_image_dir,image_name[4:])\n",
    "    \n",
    "    os.remove(os.path.join(source_image_dir,image_name[4:]))\n",
    "    os.remove(os.path.join(source_label_dir,os.path.splitext(image_name)[0][4:]+\".txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a2e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir(save_val_image_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58974966",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir(source_image_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e804d5a6",
   "metadata": {},
   "source": [
    "## YOLOv5 인퍼런스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b8d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_image_dir = '../resources/log_221017_2_narrow/images/'\n",
    "yolo_label_dir = '../resources/log_221017_2_narrow/labels/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(yolo_image_dir, exist_ok=True)\n",
    "os.makedirs(yolo_label_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a791efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "slice_size=640\n",
    "overlap_ratio=0.25\n",
    "single_row_predict=True\n",
    "single_row_y_start=200\n",
    "postprocess_match_threshold=0.5\n",
    "no_slice_prediction=True\n",
    "cnt=0\n",
    "size=(1920,1080)\n",
    "\n",
    "def convert(size, box):\n",
    "    dw = 1./(size[0])\n",
    "    dh = 1./(size[1])\n",
    "    x = (box[0] + box[1])/2.0 - 1\n",
    "    y = (box[2] + box[3])/2.0 - 1\n",
    "    w = box[1] - box[0]\n",
    "    h = box[3] - box[2]\n",
    "    x = x*dw\n",
    "    w = w*dw\n",
    "    y = y*dh\n",
    "    h = h*dh\n",
    "    return (x,y,w,h)\n",
    "\n",
    "for image_file in tqdm(sorted(glob(source_image_dir+\"/*.jpg\"))):\n",
    "    \n",
    "    shutil.copy(image_file, yolo_image_dir+os.path.basename(image_file))\n",
    "    label_file_name = os.path.splitext(os.path.basename(image_file))[0]+\".txt\"\n",
    "    \n",
    "    if not no_slice_prediction:\n",
    "        result = get_sliced_prediction(image_file,\n",
    "                                       model,\n",
    "                                       slice_height=slice_size,\n",
    "                                       slice_width=slice_size,\n",
    "                                       postprocess_match_threshold=postprocess_match_threshold,\n",
    "                                       overlap_height_ratio=overlap_ratio,\n",
    "                                       overlap_width_ratio=overlap_ratio,\n",
    "                                       single_row_y_start=single_row_y_start,\n",
    "                                       single_row_predict=single_row_predict,\n",
    "                                       verbose=0)\n",
    "    else:\n",
    "        result = get_prediction(image_file, model)\n",
    "\n",
    "    Path(yolo_label_dir+label_file_name).touch()\n",
    "    with open(yolo_label_dir+label_file_name, 'w') as f:    \n",
    "        for object_prediction in result.object_prediction_list:\n",
    "            object_prediction = object_prediction.deepcopy()\n",
    "            bbox = object_prediction.bbox.to_voc_bbox()\n",
    "            category_id = object_prediction.category.id\n",
    "            yolo = convert(size,[bbox[0], bbox[2], bbox[1], bbox[3]])\n",
    "            f.write(f\"{category_id} {yolo[0]} {yolo[1]} {yolo[2]} {yolo[3]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed46c517",
   "metadata": {},
   "source": [
    "## Visualize yolo bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8220e9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAME_TO_ID = {'Buoy': 0, 'Boat': 1, 'Channel Marker': 2, 'Speed Warning Sign': 3}\n",
    "CLASS_ID_TO_NAME = {0: 'Buoy', 1: 'Boat', 2: 'Channel Marker', 3: 'Speed Warning Sign'}\n",
    "SIZE=(1920,1080)\n",
    "\n",
    "def visualize_yolo(image, bboxes, category_ids, color=None):\n",
    "    image = copy.deepcopy(image)\n",
    "    \n",
    "    if color is None:\n",
    "        colors = Colors()\n",
    "    else:\n",
    "        colors = None\n",
    "        \n",
    "    for bbox, category_id in zip(bboxes, category_ids):\n",
    "        class_name = CLASS_ID_TO_NAME[category_id[0]]\n",
    "        \n",
    "        x_center, y_center, w, h = bbox\n",
    "        x_min = int((x_center - w/2) * SIZE[0])\n",
    "        y_min = int((y_center - h/2) * SIZE[1])\n",
    "        x_max = int((x_center + w/2) * SIZE[0])\n",
    "        y_max = int((y_center + h/2) * SIZE[1])\n",
    "\n",
    "        if colors is not None:\n",
    "            color = colors(category_id[0])\n",
    "        \n",
    "        # set rect_th for boxes\n",
    "        rect_th = max(round(sum(image.shape) / 2 * 0.001), 1)\n",
    "        # set text_th for category names\n",
    "        text_th = max(rect_th - 1, 1)\n",
    "        # set text_size for category names\n",
    "        text_size = rect_th / 3\n",
    "\n",
    "        cv2.rectangle(image, \n",
    "                      (x_min, y_min),\n",
    "                      (x_max, y_max), \n",
    "                      color=color,\n",
    "                      thickness=rect_th)\n",
    "        \n",
    "        p1, p2 = (int(x_min), int(y_min)), (int(x_max), int(y_max))\n",
    "        label = f\"{class_name}\"\n",
    "        w, h = cv2.getTextSize(label, 0, fontScale=text_size, thickness=text_th)[0]  # label width, height\n",
    "        outside = p1[1] - h - 3 >= 0  # label fits outside box\n",
    "        p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\n",
    "        \n",
    "        cv2.rectangle(image, p1, p2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            label,\n",
    "            (p1[0], p1[1] - 2 if outside else p1[1] + h + 2),\n",
    "            0,\n",
    "            text_size,\n",
    "            (255, 255, 255),\n",
    "            thickness=text_th,\n",
    "        )\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77a395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_label_files = sorted(glob(yolo_label_dir+\"/*.txt\"))\n",
    "infer_image_files = sorted(glob(yolo_image_dir+\"/*.jpg\"))\n",
    "\n",
    "@interact(index=(0,len(infer_image_files)))\n",
    "def verify_gt(index=0):\n",
    "    image = cv2.imread(infer_image_files[index])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    label = infer_label_files[index]\n",
    "    \n",
    "    bboxes = []\n",
    "    category_ids = []\n",
    "    \n",
    "    with open(label, 'r') as f:\n",
    "        line = f.readline().strip()\n",
    "        while line:\n",
    "            id, cx, cy, w, h = list(map(float, line.split()))\n",
    "            bboxes.append([cx, cy, w, h])\n",
    "            category_ids.append([int(id)])\n",
    "            line = f.readline().strip()\n",
    "\n",
    "    if len(bboxes) > 0:    \n",
    "        canvas = visualize_yolo(image, bboxes, category_ids)\n",
    "        plt.figure(figsize=(16,16))\n",
    "        plt.imshow(canvas)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf826fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efb3938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
